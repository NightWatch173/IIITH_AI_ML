## M1: Problem Formulation and Solving
	1. Representing Text and Language
		* Three Generations of Features or Representations
			1. Intuition driven representations (hand-crafted)
			2. Representations that are derived based on Statistics, Signal Processing etc
			3. Representations that are Learned
		* What Do We Mean by Representing Text?
		* Bag of Words - Text Domain
		* Bag of Words Histogram
		* Histogram of Word Occurrences
		* High level concept: 1. Implicit Biases in Machine Learning
		* High level concept: 2. Avoid Intermediate problem formulation
	2. Proxy Task - Word2Vec
		* Bag of Words
		* Power of Context
		* Humans and Machines learn fundamentally in different ways
	3. Gradient Descent
	4. Neural Networks
		* Perceptron
		* Neuron
		* MLP
	5. Activation Functions
		* Sigmoid
		* RELU
		* Leaky RELU

---


## 16th March 2019

	* Linear Regression
	* Drew a plot for x and y for linear regression
	* Moving towards gradient descent
	* Mini_Hackathon

## 17th March 2019

Lectures:
1. Lecture 1: Representing Text and Language
2. Lecture 2: Perceptrons, Neural Networks, Gradient Descent
3. Lecture 3.Learning Representations: Word2Vec
4. Lecture 4.Dimensionality Reduction

Other Topics:
	* Representing Text and Language
	* Perceptrons, Neural Networks, Gradient Descent
	* Learning Representations: Word2Vec
	* Dimensionality Reduction
	* Man - Woman + Queen = King!
	* Representing Words
	* Intuition
	* Creating the Word2Vec
	* 1-Hot to Rich Representations
	* Word Similarity = Vector Similarity
	* Representing documents(email/web page)
	* Representing Complex Entities
	* Case Study: BoW vs W2V

---

Datasets:
1. CIFAR-10 Dataset 
2. IRIS Dataset 
3. MNIST Dataset 
4. Fashion MNIST Dataset 


---





