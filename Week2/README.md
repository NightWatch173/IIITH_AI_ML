## M1: Problem Formulation and Solving
	1. Representing Text and Language
		* Three Generations of Features or Representations
			1. Intuition driven representations (hand-crafted)
			2. Representations that are derived based on Statistics, Signal Processing etc
			3. Representations that are Learned
		* What Do We Mean by Representing Text?
		* Bag of Words - Text Domain
		* Bag of Words Histogram
		* Histogram of Word Occurrences
		* High level concept: 1. Implicit Biases in Machine Learning
		* High level concept: 2. Avoid Intermediate problem formulation
	2. Proxy Task - Word2Vec
		* Bag of Words
		* Power of Context
		* Humans and Machines learn fundamentally in different ways
	3. Gradient Descent
	4. Neural Networks
		* Perceptron
		* Neuron
		* MLP
	5. Activation Functions
		* Sigmoid
		* RELU
		* Leaky RELU

---


## 16th March 2019

	* Linear Regression
	* Drew a plot for x and y for linear regression
	* Moving towards gradient descent
**Mini_Hackathon**:
* We will be using district wise demographics, enrollments, school and teacher indicator data to predict whether the literacy rate is high / medium / low in each district.
* At the end of this experiment, you will be able to: Perform Data preprocessing
* Data preprocessing is an important step of solving every machine learning problem. Most of the datasets used with Machine Learning problems need to be processed / cleaned / transformed so that a Machine Learning algorithm can be trained on it.

* There are different steps involved for Data Preprocessing. These steps are as follows:
	1. Data Cleaning → In this step the primary focus is on
		*Handling missing data
		* Handling nosiy data
		* Detection and removal of outliers
	2. Data Integration → This process is used when data is gathered from various data sources
	and data are combined to form consistent data. This data after performing cleaning is used
	for analysis.
	3. Data Transformation → In this step we will convert the raw data into a specified for-
	mat according to the need of the model we are building. There are many options used for
	transforming the data as below:
		* Normalization
		* Aggregation
		* Generalization
	4. Data Reduction → After data transformation and scaling the redundancy within the data is removed and efficiently organizing the data is performed.



## 17th March 2019

Lectures:

	1. Lecture 1: Representing Text and Language
	2. Lecture 2: Perceptrons, Neural Networks, Gradient Descent
	3. Lecture 3.Learning Representations: Word2Vec
	4. Lecture 4.Dimensionality Reduction

Other Topics:

	* Representing Text and Language
	* Perceptrons, Neural Networks, Gradient Descent
	* Learning Representations: Word2Vec
	* Dimensionality Reduction
	* Man - Woman + Queen = King!
	* Representing Words
	* Intuition
	* Creating the Word2Vec
	* 1-Hot to Rich Representations
	* Word Similarity = Vector Similarity
	* Representing documents(email/web page)
	* Representing Complex Entities
	* Case Study: BoW vs W2V

---

Datasets:

	1. CIFAR-10 Dataset 
	2. IRIS Dataset 
	3. MNIST Dataset 
	4. Fashion MNIST Dataset 


---





