{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_Mini_HCK_Data_Munging.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"QbZjZ1i5KMsv","colab_type":"text"},"cell_type":"markdown","source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"metadata":{"id":"DIfh2REWFr6p","colab_type":"text"},"cell_type":"markdown","source":["## Learning Objective"]},{"metadata":{"id":"LrIlNFZeKMsx","colab_type":"text"},"cell_type":"markdown","source":["At the end of this experiment, you will be able to:\n","\n","* Perform Data preprocessing"]},{"metadata":{"id":"H5DokZ7MF1P_","colab_type":"text"},"cell_type":"markdown","source":["## Dataset"]},{"metadata":{"id":"V3vgcWwOF2cK","colab_type":"text"},"cell_type":"markdown","source":["### Description"]},{"metadata":{"id":"fqJ1h77-KMsy","colab_type":"text"},"cell_type":"markdown","source":["We will be using district wise demographics, enrollments, school and teacher indicator data to predict whether the literacy rate is high / medium / low in each district."]},{"metadata":{"id":"md2IjdMdGCWm","colab_type":"text"},"cell_type":"markdown","source":["### Data Preprocessing"]},{"metadata":{"id":"3B5ztQVbKMsz","colab_type":"text"},"cell_type":"markdown","source":["Data preprocessing is an important step of solving every machine learning problem. Most of\n","the datasets used with Machine Learning problems need to be processed / cleaned / transformed\n","so that a Machine Learning algorithm can be trained on it."]},{"metadata":{"id":"QsxaJLZAKMs0","colab_type":"text"},"cell_type":"markdown","source":["There are different steps involved for Data Preprocessing. These steps are as follows:"]},{"metadata":{"id":"QF3Eg-5pKMs1","colab_type":"text"},"cell_type":"markdown","source":["    1. Data Cleaning → In this step the primary focus is on\n","        -Handling missing data\n","        -Handling nosiy data\n","        -Detection and removal of outliers\n","    \n","    2. Data Integration → This process is used when data is gathered from various data sources\n","    and data are combined to form consistent data. This data after performing cleaning is used\n","    for analysis.\n","    \n","    3. Data Transformation → In this step we will convert the raw data into a specified for-\n","    mat according to the need of the model we are building. There are many options used for\n","    transforming the data as below:\n","        -Normalization\n","        -Aggregation\n","        -Generalization\n","        \n","    4. Data Reduction → After data transformation and scaling the redundancy within the data\n","    is removed and efficiently organizing the data is performed.\n","\n"]},{"metadata":{"id":"Qk0G-I3iKMs2","colab_type":"text"},"cell_type":"markdown","source":["### Total Marks  = 20"]},{"metadata":{"id":"mXKT6V2zKb_v","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"bIVdqocyr3Tk","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P181902118\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tmrHzW02sDB_","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"8860303743\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GRmbeEFYGP55","colab_type":"code","cellView":"form","outputId":"10a34137-8abc-4fff-bccd-4f3df1cd88a7","executionInfo":{"status":"ok","timestamp":1552730819579,"user_tz":-330,"elapsed":8700,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAE0/PToMHu16xAo/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook=\"1_Mini_HCK_Data_Munging\" #name of the notebook\n","Answer = \"This notebook is evaluated by mentors during the lab\"\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")  \n","    #  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/data-20190108T113429Z-001.zip\")\n","    ipython.magic(\"sx unzip data-20190108T113429Z-001.zip\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", r[\"date\"])\n","      print(\"Time of submission: \", r[\"time\"])\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","    from IPython.display import HTML\n","    HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id))\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/html":["<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=P181902118&recordId=2165\"></script>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"metadata":{"id":"hZSlj_nWKMs4","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise 1 - (2 Marks)\n","We have four different files\n","\n","* Districtwise_Basicdata.csv\n","* Districtwise_Enrollment_details_indicator.csv\n","* Districtwise_SchoolData.csv\n","* Districtwise_Teacher_indicator.csv\n","These files contain the neccesary data to solve the problem.\n","Load all the files correctly, after observing the header level details, data records etc\n","\n","Hint : Use read_csv from pandas"]},{"metadata":{"id":"_XRregSb9wdB","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","import pandas as pd\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","basic_data = pd.read_csv('data/Districtwise_Basicdata.csv', skiprows=[0])\n","enroll_data = pd.read_csv('data/Districtwise_Enrollment_details_indicator.csv', skiprows=[0, 1, 2])\n","school_data = pd.read_csv('data/Districtwise_SchoolData.csv', skiprows=3)\n","teacher_data = pd.read_csv('data/Districtwise_Teacher_indicator.csv', skiprows=3)\n","\n","basic_data = basic_data.rename(index=str, columns={'Year': 'year', 'Statecd': 'statecd'})\n","enroll_data = enroll_data.rename(index=str, columns={'Year': 'year', 'Statecd': 'statecd'})\n","school_data = school_data.rename(index=str, columns={'ac_year': 'year', 'Statecd': 'statecd'})\n","teacher_data = teacher_data.rename(index=str, columns={'ac_year': 'year', 'Statecd': 'statecd'})\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aFmrZHtrpMQT","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"K2A3OKraKMs9","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise 2  - (4 Marks)\n","\n","* Remove the unwanted columns, which are unlikely to contribute for the prediction of overall literacy grade. The decision of what constitutes unwanted columns depends on how it effects your final accuracy (and very little on your domain understanding of education sector in India; you're encouraged however to exercise some domain understanding too if you wish)\n","\n","**Hint** use pandas drop function to drop your choice of unwanted columns (if any).\n","\n","\n","* As the required data is present in different files, we need to integrate all the four to make single dataframe/dataset. For that purpose, create a unique identifier for each row in all the dataframes so that it can be used to map the data in different files correctly\n","* Join/integrate this data \n","\n","Example : data of the district ananthapur in Andrapradesh, which present in different files should form a single row \n","\n","Hint : \n","* Use the combination of year, statecode, district code as unique identifier \n","\n","* Refer the following link for merge, join and concat syntaxes:  \n","\n","https://pandas.pydata.org/pandas-docs/stable/merging.html\n"]},{"metadata":{"id":"le5wGzUuKMs_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"b477a7e2-3c8f-4a2e-ad01-4b5277a98943","executionInfo":{"status":"ok","timestamp":1552736413101,"user_tz":-330,"elapsed":1378,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAE0/PToMHu16xAo/s64/photo.jpg","userId":"06632186555192968294"}}},"cell_type":"code","source":["# Your Code Here\n","\n","basic_plus_enroll = basic_data.merge(enroll_data, on=['year', 'statecd', 'distcd'])\n","basic_plus_enroll_plus_school = basic_plus_enroll.merge(school_data, on=['year', 'statecd', 'distcd'])\n","merged_data = basic_plus_enroll_plus_school.merge(teacher_data, on=['year', 'statecd', 'distcd'])\n","\n","cols_to_drop = ['Gtoilet Sch1', 'Gtoilet Sch2', 'Gtoilet Sch3', 'Gtoilet Sch4', 'Gtoilet Sch5', 'Gtoilet Sch6', 'Gtoilet Sch7', 'Gtoilet Sch']\n","cols_to_drop += ['Btoilet Sch1', 'Btoilet Sch2', 'Btoilet Sch3', 'Btoilet Sch4', 'Btoilet Sch5', 'Btoilet Sch6', 'Btoilet Sch7']\n","cols_to_drop += ['Uniform P B', 'Uniform P G', 'Uniform Up B', 'Uniform Up G']\n","cols_to_drop += ['No Fem Sch1', 'No Fem Sch2', 'No Fem Sch3', 'No Fem Sch4', 'No Fem Sch5', 'No Fem Sch6', 'No Fem Sch7', 'No Fem Sch']\n","cols_to_drop += ['Residential P B', 'Residential P G', 'Residential Up B', 'Residential Up G']\n","cols_to_drop += ['Transport P B', 'Transport P G', 'Transport Up B', 'Transport Up G']\n","cols_to_drop += ['Station P B', 'Station P G', 'Station Up B', 'Station Up G']\n","cols_to_drop += ['Computer Sch1', 'Computer Sch2', 'Computer Sch3', 'Computer Sch4', 'Computer Sch5', 'Computer Sch6', 'Computer Sch7']\n","cols_to_drop += ['Kitshed1', 'Kitshed2', 'Kitshed3', 'Kitshed4', 'Kitshed5', 'Kitshed6', 'Kitshed7']\n","cols_to_drop += ['blocks', 'clusters', 'villages', 'p_06_pop', 'sexratio_06', 'growthrate', 'p_sc_pop', 'p_st_pop', 'State Name _x', 'distname_y', 'Enr Govt1', 'Enr Govt2', 'Enr Govt3', 'Enr Govt4', 'Enr Govt5', 'Enr Govt6', 'Enr Govt7', 'Enr Govt9', 'Enr Pvt1', 'Enr Pvt2', 'Enr Pvt3', 'Enr Pvt4', 'Enr Pvt5', 'Enr Pvt6', 'Enr Pvt7', 'Enr Pvt9', 'Enr R Govt1', 'Enr R Govt2', 'Enr R Govt3', 'Enr R Govt4', 'Enr R Govt5', 'Enr R Govt6', 'Enr R Govt7', 'Enr R Govt9', 'Enr R Pvt1', 'Enr R Pvt2', 'Enr R Pvt3', 'Enr R Pvt4', 'Enr R Pvt5', 'Enr R Pvt6', 'Enr R Pvt7', 'Enr R Pvt9', 'Enr Py4 C1', 'Enr Py4 C2', 'Enr Py4 C3', 'Enr Py4 C4', 'Enr Py4 C5', 'Enr Py4 C6', 'Enr Py4 C7', 'Enr Py4 C8', 'Enr Py3 C1', 'Enr Py3 C2', 'Enr Py3 C3', 'Enr Py3 C4', 'Enr Py3 C5', 'Enr Py3 C6', 'Enr Py3 C7', 'Enr Py3 C8', 'Enr Py2 C1', 'Enr Py2 C2', 'Enr Py2 C3', 'Enr Py2 C4', 'Enr Py2 C5', 'Enr Py2 C6', 'Enr Py2 C7', 'Enr Py2 C8', 'Enr Py1 C1', 'Enr Py1 C2', 'Enr Py1 C3', 'Enr Py1 C4', 'Enr Py1 C5', 'Enr Py1 C6', 'Enr Py1 C7', 'Enr Py1 C8', 'Enr Cy C1', 'Enr Cy C2', 'Enr Cy C3', 'Enr Cy C4', 'Enr Cy C5', 'Enr Cy C6', 'Enr Cy C7', 'Enr Cy C8', 'Sc Enrp Cy', 'Sc Enrup Cy', 'Scg Enrp Cy', 'Scg Enrup Cy', 'St Enrp Cy', 'Stg Enrp Cy', 'St Enrup Cy', 'Stg Enrup Cy', 'Gerp Py2', 'Gerp Py1', 'Gerp Cy', 'Gerup Py2', 'Gerup Py1', 'Gerup Cy', 'Nerp Py2', 'Nerp Py1', 'Nerp Cy', 'Nerup Py2', 'Nerup Py1', 'Nerup Cy', 'Pc Girls1', 'Pc Girls2', 'Pc Girls3', 'Pc Girls4', 'Pc Girls5', 'Pc Girls', 'Enr G C1', 'Enr G C2', 'Enr G C3', 'Enr G C4', 'Enr G C5', 'Enr G C6', 'Enr G C7', 'Enr G C8', 'Enr Dis B C1', 'Enr Dis B C2', 'Enr Dis B C3', 'Enr Dis B C4', 'Enr Dis B C5', 'Enr Dis B C6', 'Enr Dis B C7', 'Enr Dis B C8', 'Enr Dis G C1', 'Enr Dis G C2', 'Enr Dis G C3', 'Enr Dis G C4', 'Enr Dis G C5', 'Enr Dis G C6', 'Enr Dis G C7', 'Enr Dis G C8', 'Grossness P', 'Grossness Up', 'Enr Med1 1', 'Enr Med1 2', 'Enr Med1 3', 'Enr Med1 4', 'Enr Med1 5', 'Enr Med1 6', 'Enr Med1 7', 'Enr Med2 1', 'Enr Med2 2', 'Enr Med2 3', 'Enr Med2 4', 'Enr Med2 5', 'Enr Med2 6', 'Enr Med2 7', 'Enr Med3 1', 'Enr Med3 2', 'Enr Med3 3', 'Enr Med3 4', 'Enr Med3 5', 'Enr Med3 6', 'Enr Med3 7', 'Rep C1', 'Rep C2', 'Rep C3', 'Rep C4', 'Rep C5', 'Rep C6', 'Rep C7', 'Rep C8', 'Muslim P', 'Muslim Up', 'Muslim G P', 'Muslim G Up', 'Obc P', 'Obc Up', 'Obc G P', 'Obc G Up', 'State Name _y', 'distname_x', 'schgovt1', 'schgovt2', 'schgovt3', 'schgovt4', 'schgovt5', 'schgovt6', 'schgovt7', 'schgovt9', 'schpvt1', 'schpvt2', 'schpvt3', 'schpvt4', 'schpvt5', 'schpvt6', 'schpvt7', 'schpvt9', 'Sch R Govt1', 'Sch R Govt2', 'Sch R Govt3', 'Sch R Govt4', 'Sch R Govt5', 'Sch R Govt6', 'Sch R Govt7', 'Sch R Govt9', 'Sch R Pvt1', 'Sch R Pvt2', 'Sch R Pvt3', 'Sch R Pvt4', 'Sch R Pvt5', 'Sch R Pvt6', 'Sch R Pvt7', 'Sch R Pvt9', 'Cls1 School1', 'Cls1 School2', 'Cls1 School3', 'Cls1 School4', 'Cls1 School5', 'Cls1 School6', 'Cls1 School7', 'Cls1 School', 'Tch1 School1', 'Tch1 School2', 'Tch1 School3', 'Tch1 School4', 'Tch1 School5', 'Tch1 School6', 'Tch1 School7', 'Tch1 School', 'Pp Sch1', 'Pp Sch2', 'Pp Sch3', 'Pp Sch6', 'Water Sch1', 'Water Sch2', 'Water Sch3', 'Water Sch4', 'Water Sch5', 'Water Sch6', 'Water Sch7', 'Water Sch', 'Enr Stch Sch1', 'Enr Stch Sch2', 'Enr Stch Sch3', 'Enr Stch Sch4', 'Enr Stch Sch5', 'Enr Stch Sch6', 'Enr Stch Sch7', 'Enr Stch Sch', 'Sch 50enr1', 'Sch 50enr2', 'Sch 50enr3', 'Sch 50enr4', 'Sch 50enr5', 'Sch 50enr6', 'Sch 50enr7', 'Sch 50enr', 'Sch Since 2003 1', 'Sch Since 2003 2', 'Sch Since 2003 3', 'Sch Since 2003 4', 'Sch Since 2003 5', 'Sch Since 2003 6', 'Sch Since 2003 7', 'Tot Cls1', 'Tot Cls2', 'Tot Cls3', 'Tot Cls4', 'Tot Cls5', 'Tot Cls6', 'Tot Cls7', 'Tot Cls', 'Cls Good1', 'Cls Good2', 'Cls Good3', 'Cls Good4', 'Cls Good5', 'Cls Good6', 'Cls Good7', 'Cls Good', 'Cls Major1', 'Cls Major2', 'Cls Major3', 'Cls Major4', 'Cls Major5', 'Cls Major6', 'Cls Major7', 'Cls Major', 'Cls Minor1', 'Cls Minor2', 'Cls Minor3', 'Cls Minor4', 'Cls Minor5', 'Cls Minor6', 'Cls Minor7', 'Cls Minor', 'Cls Other1', 'Cls Other2', 'Cls Other3', 'Cls Other4', 'Cls Other5', 'Cls Other6', 'Cls Other7', 'Cls Other', 'Sdg 1', 'Sdg 2', 'Sdg 3', 'Sdg 4', 'Sdg 5', 'Sdg 6', 'Sdg 7', 'Tlm 1', 'Tlm 2', 'Tlm 3', 'Tlm 4', 'Tlm 5', 'Tlm 6', 'Tlm 7', 'Book P B', 'Book P G', 'Book Up B', 'Book Up G', 'Attend P B', 'Attend P G', 'Attend Up B', 'Attend Up G', 'Sch Un1', 'Sch Un2', 'Sch Un3', 'Sch Un4', 'Sch Un5', 'Sch Un6', 'Sch Un7', 'Sch Un9',  'Electric Sch1', 'Electric Sch2', 'Electric Sch3', 'Electric Sch4', 'Electric Sch5', 'Electric Sch6', 'Electric Sch7', 'Mdm 1', 'Mdm 2', 'Mdm 3', 'Mdm 4', 'Mdm 5', 'Mdm 6', 'Mdm 7', 'Smc 1', 'Smc 2', 'Smc 3', 'Smc 4', 'Smc 5', 'Smc 6', 'Smc 7', 'App By Road 1', 'App By Road 2', 'App By Road 3', 'App By Road 4', 'App By Road 5', 'App By Road 6', 'App By Road 7', 'Scr 30 P', 'Scr 35 Up', 'Ptr 30 P', 'Ptr 35 Up', 'Avg Instn Days P', 'Avg Instn Days Up', 'statename_y', 'distname_y', 'tch_govt1', 'tch_govt2', 'tch_govt3', 'tch_govt4', 'tch_govt5', 'tch_govt6', 'tch_govt7', 'tch_govt9', 'tch_pvt1', 'tch_pvt2', 'tch_pvt3', 'tch_pvt4', 'tch_pvt5', 'tch_pvt6', 'tch_pvt7', 'tch_pvt9', 'tch_un1', 'tch_un2', 'tch_un3', 'tch_un4', 'tch_un5', 'tch_un6', 'tch_un7', 'tch_un9', 'tch_bs1', 'tch_bs2', 'tch_bs3', 'tch_bs4', 'tch_bs5', 'tch_bs6', 'tch_bs7', 'tch_bs_p', 'tch_s1', 'tch_s2', 'tch_s3', 'tch_s4', 'tch_s5', 'tch_s6', 'tch_s7', 'tch_s_p', 'tch_hs1', 'tch_hs2', 'tch_hs3', 'tch_hs4', 'tch_hs5', 'tch_hs6', 'tch_hs7', 'tch_hs_p', 'tch_grad1', 'tch_grad2', 'tch_grad3', 'tch_grad4', 'tch_grad5', 'tch_grad6', 'tch_grad7', 'tch_grad_p', 'tch_pgrad1', 'tch_pgrad2', 'tch_pgrad3', 'tch_pgrad4', 'tch_pgrad5', 'tch_pgrad6', 'tch_pgrad7', 'tch_pgrad_p', 'tch_mph1', 'tch_mph2', 'tch_mph3', 'tch_mph4', 'tch_mph5', 'tch_mph6', 'tch_mph7', 'tch_mph_p', 'tch_pd1', 'tch_pd2', 'tch_pd3', 'tch_pd4', 'tch_pd5', 'tch_pd6', 'tch_pd7', 'tch_pd_p', 'tch_eduqual_nr1', 'tch_eduqual_nr2', 'tch_eduqual_nr3', 'tch_eduqual_nr4', 'tch_eduqual_nr5', 'tch_eduqual_nr6', 'tch_eduqual_nr7', 'tch_eduqual_nr_p', 'tch_m1', 'tch_m2', 'tch_m3', 'tch_m4', 'tch_m5', 'tch_m6', 'tch_m7', 'tch_f1', 'tch_f2', 'tch_f3', 'tch_f4', 'tch_f5', 'tch_f6', 'tch_f7', 'tch_nr1', 'tch_nr2', 'tch_nr3', 'tch_nr4', 'tch_nr5', 'tch_nr6', 'tch_nr7', 'tch_m_p1', 'tch_m_p2', 'tch_m_p3', 'tch_m_p4', 'tch_m_p5', 'tch_m_p6', 'tch_m_p7', 'tch_f_p1', 'tch_f_p2', 'tch_f_p3', 'tch_f_p4', 'tch_f_p5', 'tch_f_p6', 'tch_f_p7', 'tch_nr_p1', 'tch_nr_p2', 'tch_nr_p3', 'tch_nr_p4', 'tch_nr_p5', 'tch_nr_p6', 'tch_nr_p7', 'tch_sc_m1', 'tch_sc_m2', 'tch_sc_m3', 'tch_sc_m4', 'tch_sc_m5', 'tch_sc_m6', 'tch_sc_m7', 'tch_sc_f1', 'tch_sc_f2', 'tch_sc_f3', 'tch_sc_f4', 'tch_sc_f5', 'tch_sc_f6', 'tch_sc_f7', 'tch_st_m1', 'tch_st_m2', 'tch_st_m3', 'tch_st_m4', 'tch_st_m5', 'tch_st_m6', 'tch_st_m7', 'tch_st_f1', 'tch_st_f2', 'tch_st_f3', 'tch_st_f4', 'tch_st_f5', 'tch_st_f6', 'tch_st_f7', 'trn_tch_m1', 'trn_tch_m2', 'trn_tch_m3', 'trn_tch_m4', 'trn_tch_m5', 'trn_tch_m6', 'trn_tch_m7', 'trn_tch_f1', 'trn_tch_f2', 'trn_tch_f3', 'trn_tch_f4', 'trn_tch_f5', 'trn_tch_f6', 'trn_tch_f7', 'prof_trn_tch_r', 'prof_trn_tch_p', 'days_nontch', 'tch_nontch']\n","\n","data = merged_data.drop(columns=cols_to_drop)\n","\n","print(data.head())"],"execution_count":66,"outputs":[{"output_type":"stream","text":["      year  statecd                                        statename_x  \\\n","0  2012-13       35  ANDAMAN & NICOBAR ISLANDS                     ...   \n","1  2012-13       35  ANDAMAN & NICOBAR ISLANDS                     ...   \n","2  2012-13       35  ANDAMAN & NICOBAR ISLANDS                     ...   \n","3  2012-13       28  ANDHRA PRADESH                                ...   \n","4  2012-13       28  ANDHRA PRADESH                                ...   \n","\n","   distcd  totschools  totpopulation  p_urb_pop  sexratio overall_lit  \\\n","0    3501         212       237586.0      55.89     874.0        High   \n","1    3503         181       105539.0       2.60     925.0        High   \n","2    3502          58        36819.0       0.00     778.0        High   \n","3    2801        4983      2737738.0      27.68    1003.0         Low   \n","4    2822        5188      4083315.0      28.09     977.0         Low   \n","\n","   female_lit  \n","0       84.52  \n","1       79.39  \n","2       70.70  \n","3       51.99  \n","4       54.31  \n"],"name":"stdout"}]},{"metadata":{"id":"k3wf8dFrKMtC","colab_type":"text"},"cell_type":"markdown","source":["Follow this steps in order to clean the data:"]},{"metadata":{"id":"_jcX4aRsKMtE","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise 3 - (3 Marks)\n","\n","* Overall_lit is our target variable, which we need to predict. Delete the row with missing overall_lit column\n","* Take a call to replace the missing values in any other column appropriately with mean/median/mode\n","* Convert categorical values to numerical values\n","Example : If a feature contains categorical values such as dog, cat, mouse etc then replace them with 1, 2, 3 etc or using one hot encoding (your judgement)\n","\n","*Hint* :\n","* Use pandas fillna function to replace the missing values"]},{"metadata":{"id":"REcYtu79KMtF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":311},"outputId":"2eb00258-810f-44b1-bc23-f1c5b01cf5d1","executionInfo":{"status":"ok","timestamp":1552736417606,"user_tz":-330,"elapsed":1941,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAE0/PToMHu16xAo/s64/photo.jpg","userId":"06632186555192968294"}}},"cell_type":"code","source":["# Your Code Here\n","\n","def overall_lit(n):\n","  if not type(n)==type(\"\"):\n","    return -1\n","  if n.lower()=='high':\n","    n=2\n","  elif n.lower()=='medium':\n","    n=1\n","  elif n.lower()=='low':\n","    n=0\n","  return n\n","\n","data['overall_lit']=data['overall_lit'].apply(overall_lit)\n","\n","\n","data = data[data.overall_lit != -1 ]\n","\n","data['overall_lit'].fillna((data['overall_lit'].mode()),inplace=True)\n","data.isna().sum()"],"execution_count":67,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  del sys.path[0]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["year             0\n","statecd          0\n","statename_x      0\n","distcd           0\n","totschools       0\n","totpopulation    0\n","p_urb_pop        6\n","sexratio         0\n","overall_lit      0\n","female_lit       0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":67}]},{"metadata":{"id":"SzsY-knQKMtI","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise 4 - (3 Marks)\n","\n","Use the functions below to adjust the outliers\n","\n","smooth_out function takes pandas dataframe as input and caculates mean, standard deviation of every column to check whether all the values in that lies within the range of mean +/- 2*standard_deviation of that column or not.\n","If any of the values are not present in that boundary, then that values is brought on to the boundary.\n","\n","**Hint:** Should  the index column be normalized too? \n","\n","<img src=\"https://cdn.talentsprint.com/aiml/Experiment_related_data/normal_dist.png\">"]},{"metadata":{"id":"uhdy63OSKMtK","colab_type":"code","colab":{}},"cell_type":"code","source":["# Function to clip and clam the data\n","def clip_clamp(x, mean, sd):\n","    # Checking whether the value is less than a differenced value between mean and standard deviation.\n","    if x < mean - 2*sd :\n","        return mean - 2*sd\n","    #Checking whether the value is greater than a differenced value between mean and standard deviation.\n","    elif x > mean + 2*sd :\n","        return mean + 2*sd\n","    # If above two conditions are not statisfied we will return the original value\n","    else :\n","        return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YXWSJ2wCKMtP","colab_type":"code","colab":{}},"cell_type":"code","source":["# Function to smooth the data\n","def smooth_out(Total_data):\n","    for i in Total_data.columns:\n","        # Calculating the mean value\n","        mean = np.mean(Total_data[i].values, axis=0)\n","        # Calculating the standard deviation value\n","        sd = np.std(Total_data[i].values, axis=0)\n","        # Calculating the corrected value using clip and clamp function\n","        corrected = np.array([clip_clamp(x, mean, sd) for x in Total_data[i].values])\n","        # Storing the data in form of series\n","        Total_data[i] = pd.Series(corrected, index=Total_data[i].index)\n","    return Total_data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-hU2poUeKMtT","colab_type":"code","colab":{}},"cell_type":"code","source":["# Your Code Here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-dMbvU-KKMtY","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise 5 - (2 Marks)\n","\n","Use the function below (corr_features) to identify uncorrelated features and remove the remaining features\n","* corr_features takes pandas dataframe, columns in the dataframe and bar (corelation co-efficient)"]},{"metadata":{"id":"kgJeGouOKMtZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"948248dd-3d58-4491-8b46-1c391f63108c","executionInfo":{"status":"ok","timestamp":1552736425184,"user_tz":-330,"elapsed":1712,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAE0/PToMHu16xAo/s64/photo.jpg","userId":"06632186555192968294"}}},"cell_type":"code","source":["# Function to find uncorrelated features\n","def corr_features(df,cols,bar=0.9):\n","    for c,i in enumerate(cols[:-1]):\n","        col_set = set(cols)\n","        for j in cols[c+1:]:\n","            if i==j:\n","                continue\n","           \n","            score = df[i].corr(df[j])\n","            \n","            if score>bar:\n","                cols = list(col_set-set([j]))\n","            if score<-bar:\n","                cols = list(col_set-set([j]))\n","    return cols\n","\n","\n","\n","numeric_cols = []\n","for col in data.columns:\n","    try:\n","        if type(data[col][0]) != str:\n","            numeric_cols.append(col)\n","    except:\n","        pass\n","# print(numeric_cols)\n","filtered_cols = corr_features(data, numeric_cols)\n","print(filtered_cols)\n"],"execution_count":68,"outputs":[{"output_type":"stream","text":["['statecd', 'sexratio', 'totschools', 'p_urb_pop', 'totpopulation', 'female_lit', 'overall_lit']\n"],"name":"stdout"}]},{"metadata":{"id":"5njOPWIXKMtd","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise 6 - (3 Marks)\n","\n","Perform Mean Correction and Standard Scaling on the data feature/column wise.\n","\n","**Hint:** In order to understand the idea behind the terms used above, you may refer the following link: \n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"]},{"metadata":{"id":"988wdlpDKMtd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1143},"outputId":"a344eac3-644c-47dc-d829-91fe7d9fd619","executionInfo":{"status":"ok","timestamp":1552736430215,"user_tz":-330,"elapsed":1159,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAE0/PToMHu16xAo/s64/photo.jpg","userId":"06632186555192968294"}}},"cell_type":"code","source":["\n","tmp_df = df.select_dtypes(exclude=['object'])\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler(copy = False)\n","print(scaler.fit(tmp_df))\n","print(scaler.mean_)"],"execution_count":69,"outputs":[{"output_type":"stream","text":["StandardScaler(copy=False, with_mean=True, with_std=True)\n","[1.71087613e+01 1.72739124e+03 1.09166994e+03 2.67594411e+02\n"," 5.77643505e+00 1.81322727e+02 4.37802115e+01 1.58328290e+01\n"," 4.11470811e+01 6.50529501e-01 1.69873867e+02 1.29706949e+02\n"," 4.14863636e+01 3.99629349e+01 2.96301059e+01 3.34478852e+01\n"," 3.04198185e+01 4.47987851e-02 1.02338066e+03 2.42568405e+02\n"," 3.23222390e+00 1.71014372e+02 3.53850227e+01 1.42662632e+01\n"," 3.59750755e+01 6.98027314e-02 1.15727341e+02 7.36465257e+01\n"," 1.93547655e+01 3.31014383e+01 1.90605144e+01 1.82900302e+01\n"," 2.03413897e+01 2.95230886e-02 9.69478852e+01 2.53247734e+00\n"," 3.90778534e-01 4.00453172e+00 1.31873112e+00 4.09365559e-01\n"," 5.11858006e+00 1.10756798e+02 1.50400302e+02 4.73187311e+00\n"," 3.75377644e-01 2.73776435e+01 7.36404834e-01 3.04380665e-01\n"," 6.25377644e-01 1.84563444e+02 0.00000000e+00 0.00000000e+00\n"," 0.00000000e+00 0.00000000e+00 8.76871526e+01 9.33460045e+01\n"," 9.01632704e+01 8.11135423e+01 8.81542447e+01 8.67958837e+01\n"," 7.79562387e+01 9.00857175e+01 1.21481647e+03 4.02596677e+02\n"," 4.86759819e+01 2.08466012e+02 7.29977341e+01 5.08972810e+01\n"," 7.01321752e+01 2.06876586e+03 8.14218127e+03 6.12039275e+02\n"," 1.21749245e+02 2.11970997e+03 1.11122356e+02 6.99622356e+01\n"," 7.00845921e+01 1.12474486e+04 3.32327795e+02 6.85203927e+01\n"," 4.02719033e+00 7.29395770e+01 1.07469789e+01 6.78927492e+00\n"," 1.43595166e+01 5.09820997e+02 5.39560423e+02 3.58874622e+01\n"," 2.32350718e+00 6.25551360e+01 7.58081571e+00 2.38595166e+00\n"," 1.39592145e+01 6.65311934e+02 1.94086103e+02 1.03950151e+01\n"," 4.72809668e-01 8.70770393e+01 4.76963746e+00 6.27643505e-01\n"," 4.43731118e+00 7.07425227e+02 1.87633686e+02 1.75037821e+00\n"," 1.08058157e+02 2.50279456e+01 1.01102719e+01 1.97749245e+01\n"," 4.52530136e+03 2.93206042e+03 6.76305136e+02 8.79522659e+02\n"," 5.13337613e+02 4.86462991e+02 3.69151813e+02 5.75528701e-01\n"," 3.49240634e+03 2.45623943e+03 6.57314199e+02 6.94996224e+02\n"," 4.39015106e+02 4.43337613e+02 2.98777190e+02 5.36253776e-01\n"," 4.04242447e+02 1.86990181e+02 5.60498489e+00 6.90906344e+01\n"," 3.05445619e+01 1.58874622e+01 3.00951662e+01 9.81873112e-03\n"," 6.28590634e+02 2.87163897e+02 1.60559335e+01 1.15841390e+02\n"," 4.95135952e+01 2.91570997e+01 4.38361027e+01 3.24773414e-02\n"," 4.95351208e+02 3.15616314e+02 1.81617536e+02 8.25475831e+01\n"," 2.31853474e+02 8.38111782e+01 1.28018882e+02 2.12396070e-01\n"," 8.81355136e+01 6.11739048e+01 2.17890332e+01 6.34776435e+01\n"," 3.87495997e+01 3.47075076e+01 4.04561631e+01 8.38652039e+01\n"," 5.81912462e+01 2.13927341e+01 6.01544898e+01 3.73054230e+01\n"," 3.46167749e+01 4.01367120e+01 6.22085181e+04 6.33094471e+04\n"," 3.22672153e+04 3.35745823e+04 2.19619894e+04 2.42961488e+04\n"," 1.58101843e+04 2.11680650e+04 8.37669562e+03 9.79681495e+03\n"," 5.11529154e+03 5.63967221e+03 1.75823285e+04 2.16949433e+04\n"," 9.95837613e+02 1.20640257e+03 1.71464562e+02 1.78226956e+02\n"," 2.21519615e+02 2.58773497e+02 3.31491488e+02 3.28165544e+02\n"," 2.23037808e+02 4.94566465e+02 1.57712236e+02 2.82698413e+01\n"," 4.38157100e+01 2.93885110e+01 2.31842900e+01 3.97628399e+01\n"," 6.03799094e+01 8.72968278e+01 5.02121148e+01 5.63711480e+01\n"," 6.32327719e+01 4.72807855e+01 6.35704909e+01 5.23842630e+02\n"," 1.55193527e+02 2.58423716e+02 2.20581571e+02 4.09010574e+01\n"," 4.65200302e+01 5.70475831e+01 5.24234139e+02 1.50781392e+02\n"," 5.76148036e+01 1.68663142e+02 3.60657596e+01 2.47288520e+01\n"," 5.07560423e+01 5.78182918e+02 2.03928949e+02 5.45903323e+02\n"," 2.34369335e+02 3.12734139e+01 1.10177627e+02 6.48043807e+01\n"," 2.80641239e+02 1.51369894e+02 7.57963746e+02 1.94774924e+02\n"," 1.12768707e+02 1.94938822e+02 4.23851964e+01 1.04697885e+01\n"," 1.97413011e+01 1.10012840e+03 3.60740181e+02 4.86540785e+01\n"," 2.04018127e+02 8.24637462e+01 4.32371601e+01 5.05679758e+01\n"," 6.02184290e+02 2.80206949e+02 6.79138973e+02 2.39462236e+02\n"," 2.22995468e+02 2.24481873e+02]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  return self.partial_fit(X, y)\n"],"name":"stderr"}]},{"metadata":{"id":"QugqX021KMtl","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise 7 - (3 Marks)\n","\n","Apply different classifiers on the preprocessed data and figure out which classifier gives the best result."]},{"metadata":{"id":"s6ka-fmz8gGj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"df00d4db-b910-4cb5-ebbb-c7ff63c45065","executionInfo":{"status":"ok","timestamp":1552736722664,"user_tz":-330,"elapsed":1125,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAE0/PToMHu16xAo/s64/photo.jpg","userId":"06632186555192968294"}}},"cell_type":"code","source":["\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import linear_model\n","\n","\n","#KNN Function\n","def callKnn(data,targets):\n","  k = math.sqrt(data.shape[0])\n","  X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.2, random_state=42)\n","  neigh = KNeighborsClassifier(n_neighbors=3)\n","  neigh.fit(X_train, y_train)\n","  predicted_labels = neigh.predict(X_test)\n","  return accuracy_score(y_test,predicted_labels)\n","\n","\n","# Decision Tree\n","def callDecisionTree(data, targets):\n","  X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.2, random_state=42)\n","  decision_tree = DecisionTreeClassifier(max_depth=7)\n","  decision_tree.fit(X_train,y_train)\n","  decision_tree.predict(X_test)\n","  return decision_tree.score(X_test,y_test)\n","\n","#Linear Class func\n","def callLinearClassifier(data, targets):\n","  X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.2, random_state=42)\n","  linear_classifier = linear_model.SGDClassifier()\n","  linear_classifier.fit(X_train,y_train)\n","  linear_classifier.predict(X_test)\n","  return linear_classifier.score(X_test,y_test)\n","\n","\n","tmp_df = df.select_dtypes(exclude=['object'])\n","data \n","print(callKnn(data[['totschools', 'totpopulation', 'sexratio', 'female_lit']], data['overall_lit']))\n","print(callDecisionTree(data[['totschools', 'totpopulation', 'sexratio', 'female_lit']], data['overall_lit']))\n","print(callLinearClassifier(data[['totschools', 'totpopulation', 'sexratio', 'female_lit']], data['overall_lit']))\n","\n"],"execution_count":80,"outputs":[{"output_type":"stream","text":["0.468503937007874\n","0.9566929133858267\n","0.48031496062992124\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n","  FutureWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"3Kj2TR53KMtq","colab_type":"text"},"cell_type":"markdown","source":["### Replace any of the above given functions and get correct results to get excellence"]},{"metadata":{"id":"5lIg1Z7SO_BB","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \" \" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oTVihs7yPA9U","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \" \" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_h4mptMUPDjL","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \" \" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xia2SpE9PFb3","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RV1BegGAPKdL","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}