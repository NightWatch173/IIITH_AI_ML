{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MiniHackathon_III_StackOverflow_Classification_Clustering.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["O4mJKZzgrMLp","9fYwJv9T9--K"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"74lzxaiQ9-9s"},"source":["\n","\n","\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"O4mJKZzgrMLp","colab_type":"text"},"source":["## Problem Statement"]},{"cell_type":"markdown","metadata":{"id":"kiWKLfqkrOB6","colab_type":"text"},"source":["\n","The problem as a part of this Mini-hackathon, is for you to be able to classify a given pair of stackoverflow posts, as duplicate, direct-link, indirect-link or unrelated. The features will be the word2vec average representation of the two posts, and the label will 4 possibilities mentioned above.\n","\n","\n","** You will be graded on the following activities as a part of this larger problem: **\n","\n","1.   Using pre-trained Word2Vec model, to get a word2vec representation of the 'massaged' stackoverflow data (i.e. no preprocessing would be required)\n","2.   Building classifiers using SKLearn and reporting accuracies.\n","3.   We have provided an architecture such that the entire data can be clustered, and classifiers can run on each cluster using multithreading. Your task is to only create a KMeans clustering definition using SKLearn, and plug it at the location asked.\n","\n","** The objective of this experiment below is to show alternate possibilities of classifications using good representations and using concepts such as multithreading over clusters. You're thus combining, classification, clustering (enabled by multi-threading) ** . Further the foundation code would also give an insight into how to build generic-code so that you can run multiple classifiers and compare the results.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tzgxavYb-pDg","colab_type":"text"},"source":["## VERY IMP NOTE: \n","1. Do ensure you are using Python 3. \n","2. GPU is not needed.\n","3. There is a lot of code here below, however please narrow down to exactly what is being asked. (that will help you score on the tasks while gradually understanding the pieces of the code.) ** Also it is recommended to read this notebook bottom-up, so that you start from what is asked and then get into details. Also the code is arranged the same way **\n","4.  Please be clinical in your implementation. Make necessary copies before making edits (as there are several lines of code and might get difficult to track changes). Avoid \"Reset all runtimes\" (as this will force you download entire word2vec again; \"Restart runtime\" is better as it doesn't refresh the drive i.e. all the files that you downloaded will still be present, and only the session variables will be refreshed) ; also be very sure about your changes before running the last KMeans section (else you might have to keep waiting for about 10 to 15 mins each time) \n","5.  Finally please look for the graded problems in the last cell of this notebook; That will further guide you to relevant sections in the notebook that you need to fill"]},{"cell_type":"markdown","metadata":{"id":"QIl5bg9HWXDq","colab_type":"text"},"source":["## Data\n","\n","<pre>\n","\n","The original data is stored in ASEDataset folder.\n","\n","├── ASEDataset\n","│   ├── _4_SentenceData.txt\n","│   ├── testPair.txt\n","│   └── trainingPair.txt\n","\n","\n"," scores in testPair.txt    ID\t    type\n"," \n","    1.00              |    1     |  duplicate\n","    0.8                |    2     |  direct link\n"," btwn 0 / 0.9         |    3     |  indirect link\n","    0.00              |    4     |  isolated\n","The structure of the training/testing data is stored in pandas.dataframe format.\n","\n",">>> train_pd.head\n","ID,    'PostId', 'RelatedPostId', 'LinkTypeId', 'PostIdVec', 'RelatedPostIdVec', 'Output'\n","0       283           297          1              [...]         [...]              [...]\n","1        56            68          2              [...]         [...]              [...]\n","2         5            16          3              [...]         [...]              [...] \n","3      9083          6841          4              [...]         [...]              [...]   \n","4      5363          5370          1              [...]         [...]              [...]  \n","5       928           949          2              [...]         [...]              [...]  \n","...\n","'PostIdVec' and 'RelatedPostIdVec' represent the word embeddings of corresponding posts.\n","\n","The relationship between 'PostId' (or 'Related PostId') and Sentence in '_4_SentenceData.txt' is :\n","\n","The Nth line of _4_SentenceData.txt is the text of the question whose Id is N+1. For example,\n","\n","PostId: 283, refers to the 283th line of text in _4_SentenceData.txt \n","RelatedPostId: 297, refers to the 296th line of text in _4_SentenceData.txt \n","LinkTypeId:1, menas that they're duplicate.\n","\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"sJBbfrbb_4Z-","colab_type":"text"},"source":["## Code walkthrough:\n","\n","Here is a short code walkthrough, by explanation of all the pieces below (there are code comments within the code as well. Those might not be covered below). \n","\n","-   Firstly we download the word2vec embedding an unzip them. For this we have the ** prepare_word2vec() ** method </br>\n","-   We then call the classifier code (or for the last section clustering code) such as ** run_MLP  or run_KNN_baseline or run_SVM_baseline **\n",">-  In either of the three above, the logic is the same and is as follows:\n",">>-  Create a word2vec representation of your data i.e. the textual post related to postId and relatedPostId is taken, and a word to vec represenation is crated out of it. Further, the average of these two representation is your feature vecture; and the fact that they are duplicate, direct-link etc are labels. You have to code this part under **get_document_vec** method. (Look out for \"YOUR CODE BELOW\" tag)\n",">>- After the above is done, train-test split is performed and a classifer (using SKlearn) has to be defined. The variable ** clf ** will hold the classifier and that has to be defined by you with the parameters. (Start off with the most basic.)\n","-   ** run_kmeans_m ** is different from the above in that, after loading the data in a similar fashion, instead of calling a single classifier, the data is first clustered to multiple clusters, and for each cluster a classifier is called (using Python threading) . Read the code comments on the function."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9fYwJv9T9--K"},"source":["## Setup Steps"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tOGbD4Wa9--N","colab":{}},"source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P181902118\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BQf6sWOR9--S","colab":{}},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"8860303743\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","cellView":"form","id":"HPdGmFba9--f","outputId":"f74ea46d-1c43-4e24-ab11-83b82fddc843","executionInfo":{"status":"ok","timestamp":1556963102791,"user_tz":-330,"elapsed":15955,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"M1_Mini_Hackathon_AuthorIdentification\" #name of the notebook\n","Answer = \"This notebook is not graded\"\n","def setup():\n","#  ipython.magic(\"sx pip3 install wget\")\n","   ipython.magic(\"sx wget https://www.dropbox.com/s/9d7r7m3q85met13/Mini_hackathon_III.zip?dl=1\")\n","   ipython.magic(\"sx unzip Mini_hackathon_III.zip?dl=1\")\n","   print (\"Setup completed successfully\")\n","   return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", r[\"date\"])\n","      print(\"Time of submission: \", r[\"time\"])\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ioUZ8pJa9--n"},"source":["### NOTE: Please feel free to use ML libraries such as Sklearn  etc whereever applicable"]},{"cell_type":"markdown","metadata":{"id":"nO2zIudGdZ0R","colab_type":"text"},"source":["## Ensure you change to the directory below"]},{"cell_type":"code","metadata":{"id":"4IsfU5f8YrNu","colab_type":"code","outputId":"1b342084-1099-4a2e-a751-f1addde383b5","executionInfo":{"status":"ok","timestamp":1556963102792,"user_tz":-330,"elapsed":15938,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%cd Mini_hackathon_III/"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/Mini_hackathon_III\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aKHbvqvw-im8","colab_type":"code","outputId":"974a908b-b271-429b-fbb4-7fb89f8829b3","executionInfo":{"status":"ok","timestamp":1556963106022,"user_tz":-330,"elapsed":19148,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pwd"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/Mini_hackathon_III\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZXleKuISYimW","colab_type":"code","outputId":"13562ee3-84ed-40b6-df0d-fa9caeac603a","executionInfo":{"status":"ok","timestamp":1556963109900,"user_tz":-330,"elapsed":23012,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pip install wget"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gLIWMY7AUsVa","colab_type":"text"},"source":["## ALERT: Your code change expected in the cell below (look for the tag: \" YOUR CODE HERE \")"]},{"cell_type":"code","metadata":{"id":"bKT9kuVY658Z","colab_type":"code","colab":{}},"source":["import warnings\n","import os\n","import wget\n","import zipfile\n","import random\n","import numpy as np\n","import gensim\n","import pandas as pd\n","import threading\n","from sklearn import metrics\n","\n","#used by k-means\n","from multiprocessing import Queue\n","from sklearn.cluster import KMeans\n","\n","#importing models\n","from sklearn import svm\n","\n","#related to training algorithm\n","from learners import SK_SVM,SK_KNN,SK_MLP\n","from helpers import tune_learner#this takes care of importing tune_learner method\n","#from tuner import DE_Tune_ML\n","from sklearn.model_selection import StratifiedKFold\n","\n","#Generic\n","import timeit\n","from utility import study\n","\n","from __future__ import division\n","import pdb\n","import os\n","import numpy as np\n","import pandas as pd\n","import pickle\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ptkmFlviaxbj","colab_type":"text"},"source":["NOTE: \n","In the cell above the imports for 'learners', 'helpers' etc pertain to local modules. Here are some FAQs:</br>\n","-   If you run the command \"%ls \" you will see a few .py files. These are the local modules. \n","-   If you intend to read them, you can either download and open in your editer. Or use \"%cat fileName.py\"\n","-   You can further edit it (by using a local editor after downloading file) and upload it back. However ensure you do run the command \"%run fileName.py\" for your session to reload the file into the colab runtime (editing file on disk is not the same as having the file loaded on runtime)"]},{"cell_type":"code","metadata":{"id":"7c4dqTr_-2ah","colab_type":"code","outputId":"29387165-8ff6-45a5-890d-f51522334fb2","executionInfo":{"status":"ok","timestamp":1556963112687,"user_tz":-330,"elapsed":25775,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["ASEDataset  learners.py  __pycache__  utility.py\tword2vecs_models.zip\n","helpers.py  newabcd.py\t tuner.py     word2vecs_models\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UWN8c3PWqKFL","colab_type":"code","colab":{}},"source":["def get_acc(cm):\n","  out = []\n","  for i in range(4):\n","    out.append(cm[i][i] / 400)\n","  return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vCk3wiF0rD1K","colab_type":"code","colab":{}},"source":["\n","def load_vec(d, data, use_pkl=False, file_name=None):\n","  if use_pkl:\n","    if os.path.isfile(file_name):\n","      with open(file_name, \"rb\") as my_pickle:\n","        return pickle.load(my_pickle)\n","  else:\n","    # print(\"call get_document_vec\")\n","    return d.get_document_vec(data, file_name)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8jvsVm-mk51","colab_type":"text"},"source":["###PaperData"]},{"cell_type":"code","metadata":{"id":"DmPzDbPX10Xu","colab_type":"code","colab":{}},"source":["class PaperData(object):\n","  def __init__(self, word2vec, dir=\"ASEDataset\" + os.path.sep):\n","    self.word2vec = word2vec\n","    self.sentences = self.load_sentences(dir + \"_4_SentenceData.txt\")\n","    self.train_src = dir + \"trainingPair.txt\"\n","    self.test_src = dir + \"testPair.txt\"\n","    self.train_data = self.load_data(self.train_src)\n","    self.test_data = self.load_data(self.test_src)\n","\n","  def load_data(self, dir):\n","    result = pd.DataFrame()\n","    data = pd.read_table(dir)\n","    i = 0\n","    while i < len(data):\n","      result = self.save(result, pd.DataFrame(\n","        [[data.iloc[i, 0], data.iloc[i + 1, 0], data.iloc[i + 3, 0]]],\n","        columns=[\"PostId\", \"RelatedPostId\", \"LinkTypeId\"]))\n","      i += 5\n","    return result\n","\n","  def load_sentences(self, dir):\n","    result = pd.read_table(dir, sep=\"\\n\", header=None)\n","    return result\n","  \n","  def save(self, out, result):\n","    if out.empty:\n","      out = result\n","    else:\n","      out = out.append(result, ignore_index=True)\n","    return out\n","\n","  def get_document_vec(self, data, file_name=\"\"):\n","    \"\"\"\n","    this will get the word vector representations for original posts and realted\n","    posts, and store the results back to data, which is in pandas dataframe.\n","\n","    For example, the results will have the following columns.\n","\n","    \"PostId\",\"RelatedPostId\",\"LinkType\",\"PostIdVec\",\"RelatedPostIdVec\",\"Output\"\n","\n","    Output will be the mean of corresponding \"PostIdVec\" and\n","    \"RelatedPostIdVec\"\"\n","\n","    mappings between labels and id's\n","\n","    1.00              |    1     |  duplicate\n","    0.8               |    2     |  direct link\n","    0<x<0.8           |    3     |  indirect link\n","    0.00              |    4     |  isolated\n","\n","\n","    :param data: pandas.DataFrame of train or test data\n","    :return: None\n","    \"\"\"\n","    if len(data) <= 0 or not isinstance(data, pd.DataFrame):\n","      raise ValueError(\n","        \"Please generate {0} in pandas.dataframe first!\".format(\n","          str(data)))\n","    \n","    if not self.word2vec:\n","      raise ValueError(\"Please load pre-trained word2vec mode first!\")\n","    \n","    data[\"PostIdVec\"] = \"\"\n","    data[\"RelatedPostIdVec\"] = \"\"\n","    \n","    for column in [\"RelatedPostId\", \"PostId\"]: \n","      rows = list(map(int, data[column].tolist())) #Changed to a list\n","      pd_posts = self.sentences.iloc[rows, 0]\n","      for index, post_sentences in enumerate(pd_posts.tolist()):  \n","        key_list = post_sentences.split(\" \")\n","        x = []\n","        #YOUR CODE HERE .START (NOTE: key_list above contains the sentences. x below is word to vec representation of ...\n","        #...each word of the sentence) And self.word2vec is the word2vec model which you can use to get your representations\n","        for each in key_list:\n","          try:\n","            x.append(self.word2vec[each])\n","          except:\n","            print(\"\", end = \"\")\n","        \n","        word_count = len(x)\n","        word_vecs = np.sum(x, axis=0)\n","        data.set_value(index, column + \"Vec\", word_vecs / word_count)\n","        \n","        # YOUR CODE HERE END\n","        \n","    \n","    data[\"Output\"] = (data[\"PostIdVec\"] + data[\"RelatedPostIdVec\"]) / 2\n","    if file_name:\n","      with open(file_name, \"wb\") as mypickle:\n","        pickle.dump(data, mypickle)\n","    return data\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mKftzrwumqHz","colab_type":"text"},"source":["###SVM"]},{"cell_type":"code","metadata":{"id":"OVzvxD_1iiV8","colab_type":"code","colab":{}},"source":["def results_SVM_C(predicted, test_Y):  \n","  # labels: [\"Duplicates\", \"DirectLink\",\"IndirectLink\", \"Isolated\"]\n","  report_gen = metrics.classification_report(\n","      test_Y, predicted, labels=[\"1\", \"2\", \"3\", \"4\"], digits=3)\n","  print(report_gen)\n","  return \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFkmKfsJX8O9","colab_type":"code","colab":{}},"source":["#@study is a Python decorator used internally. It is not relevant to your task, and hence you can skip understanding it.\n","@study\n","def run_tuning_SVM_C(\n","                   #word2vec_src,\n","                   train_pd_c,\n","                   queue,l,\n","                   test_pd_c,\n","                   repeats=1,\n","                   fold=2,\n","                   tuning=True):\n","  \"\"\"\n","  :param word2vec_src:str, path of word2vec model\n","  :param repeats:int, number of repeats\n","  :param fold: int,number of folds\n","  :param tuning: boolean, tuning or not.\n","  :return: None\n","  \"\"\"\n","  #print(\"# word2vec:\", word2vec_src)\n","  #word2vec_model = gensim.models.Word2Vec.load(word2vec_src)\n","  #data = PaperData(word2vec=word2vec_model)\n","  train_pd_c = train_pd_c.reset_index()\n","  train_pd = train_pd_c\n","  test_pd = test_pd_c\n","  # NOTE: The line below merely tries to 'substitue' the string 'learner' with the string 'SK_SVM'. SK_SVM is the wrapper around ..\n","  # ...the SVM classifier. This wrapper is needed because in addition to fitting classifier it also defines methods to report scores\n","  # ..and accuracies.\n","  learner = [SK_SVM][0] \n","  goal = {0: \"PD\", 1: \"PF\", 2: \"PREC\", 3: \"ACC\", 4: \"F\", 5: \"G\", 6: \"Macro_F\",\n","          7: \"Micro_F\"}[6]\n","  F = {}\n","  clfs = []\n","  for i in range(repeats):  # repeat n times here\n","#    kf = StratifiedKFold(train_pd.loc[:, \"LinkTypeId\"].values, fold,\n"," #                        shuffle=True)\n","  #  for train_index, tune_index in kf:\n","    kf = StratifiedKFold(n_splits=fold, random_state=None, shuffle=True)\n","    for train_index, tune_index in kf.split(train_pd.loc[:, \"Output\"].values,train_pd.loc[:, \"LinkTypeId\"].values):\n","\n","      train_data = train_pd.ix[train_index]\n","      tune_data = train_pd.ix[tune_index]\n","      train_X = train_data.loc[:, \"Output\"].values\n","      train_Y = train_data.loc[:, \"LinkTypeId\"].values\n","      tune_X = tune_data.loc[:, \"Output\"].values\n","      tune_Y = tune_data.loc[:, \"LinkTypeId\"].values\n","      test_X = test_pd.loc[:, \"Output\"].values\n","      test_Y = test_pd.loc[:, \"LinkTypeId\"].values\n","      #tuner_learner below internally also tries for find the best paramters of running SVM  \n","      #using DE classes (#You will find the details of DE classes of this in learners.py. DE standads for 'Differential Evolution'; it is a \n","      # mechanism used to find the best parameters to run the classifier with. You may ignore the details of this during the hackathon).\n","      #..Once it is found it is passes as a parameter to your classifer 'learner' ('learner' is explained early in this cell)\n","      \n","      params, evaluation = tune_learner(learner, train_X, train_Y, tune_X,\n","                                        tune_Y, goal) if tuning else ({}, 0)\n","      clf = learner(train_X, train_Y, test_X, test_Y, goal)\n","      F = clf.learn(F, **params)\n","      clfs.append(clf)\n","  clfs.append(l)    \n","  queue.put(clfs)\n","  return clfs\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZoDi52PEUgg_","colab_type":"text"},"source":["### run_kmeans_m"]},{"cell_type":"code","metadata":{"id":"9ju6aH9V6Qcg","colab_type":"code","colab":{}},"source":["def run_kmeans_m(train_pd, test_pd):\n","\n","#   print(\"# word2vec:\", word2vec_src)\n","#   word2vec_model = gensim.models.Word2Vec.load(word2vec_src)\n","#   data = PaperData(word2vec=word2vec_model)\n","#   train_pd = load_vec(data, data.train_data, use_pkl=False)\n","#   test_pd = load_vec(data, data.test_data, use_pkl=False)\n","  train_X = train_pd.loc[:, \"Output\"].tolist()\n","  # Each cluster and the classifier which is being trained on it pushed to the 'queue' created below\n","  queue = Queue()\n","\n","  \n","  #<YOUR CODE HERE --START>\n","  numClusters = 4 #Experiment with different clusters.\n","  from sklearn.cluster import KMeans\n","  clf = KMeans(n_clusters=numClusters, random_state=0)\n","  #clf = Define a Sklearn kmeans cluster with the parameters above\n","  #<YOU CODE END>\n","  \n","  \n","  \n","  start0 = timeit.default_timer()\n","  #YOUR CODE HERE: Fit the K-means clustering defined above.\n","  clf.fit(train_X)\n","  stop0 = timeit.default_timer()\n","\n","  \n","  svm_models = []  # maintain a list of svms\n","  s1 = timeit.default_timer()\n","  data.train_data['clabel'] = clf.labels_\n","  s2 = timeit.default_timer()\n","  print(\"Inter - \", (s2-s1))\n","  start1 = timeit.default_timer()\n","  # the run_tuning_SVM_C function below takes care of training the SVM classifier for the passed dataset using KFold.\n","  target_model = run_tuning_SVM_C\n","  #For each of the clusters created above, we create a classifier (here it is an SVM inside run_tuning_SVM_C) and pass the..\n","  #..cluster data as input paramter\n","  for l in range(numClusters):\n","    cluster = data.train_data.loc[data.train_data['clabel'] == l] \n","    print(\"Thread No\", l)\n","    #result.append(pool.apply_async(run_KNN_C, args = (cluster,queue,)))\n","    t = threading.Thread(target = run_tuning_SVM_C, args = [cluster,queue,l,test_pd])\n","    threads.append(t)\n","    #we now start the threads (until this point though the threads are setup above the process doens't start)\n","  for th in threads:\n","      th.start()\n","    # now we hear back from each of the thread and ensure it completes; i.e. we ensure that the classifier has finished running...\n","    #.. on the particular cluster\n","  for th in threads:\n","      response = queue.get()\n","      svm_models.append(response)   \n","  #in the rest of the code below, we take each of the returned classifier and run it on test-data (spefic to each cluster)...\n","  #.. to determine the accuracies\n","  svm_models = sorted(svm_models, key = lambda th: th[-1] )    \n","  stop1 = timeit.default_timer()  \n","  svm_results = [] # maintain a list of svm results\n","  test_X = test_pd.loc[:, \"Output\"].tolist()\n","  predicted = clf.predict(test_X)\n","  data.test_data['clabel'] = predicted\n","  total_predicted = []\n","  total_cluster_Y = []\n","  avg_predicted = []\n","  avg_cluster_Y = []\n","  print(\"len(svm_models[l])-1=\", len(svm_models[l])-1)\n","  \n","  for i in range(len(svm_models[l])-1):\n","    total_predicted = []\n","    total_cluster_Y = []\n","    print(\"numClusters=\", numClusters)\n","    for l in range(numClusters):\n","      cluster = data.test_data.loc[data.test_data['clabel'] == l]\n","      svm_model = svm_models[l][i]\n","      cluster_X = cluster.loc[:, \"Output\"].tolist()\n","      cluster_Y = cluster.loc[:, \"LinkTypeId\"].tolist()\n","      total_cluster_Y = np.append(total_cluster_Y,cluster_Y)\n","      avg_cluster_Y = np.append(avg_cluster_Y,cluster_Y)\n","      if target_model == run_tuning_SVM_C or target_model == run_tuning_KNN_C:\n","          predicted_C = svm_model.learner.predict(cluster_X)\n","      else:\n","          predicted_C = svm_model.predict(cluster_X)\n","      total_predicted = np.append(total_predicted,predicted_C)\n","      avg_predicted = np.append(avg_predicted,predicted_C)\n","    svm_results.append(results_SVM_C(total_predicted, total_cluster_Y))# store all the SVM result report in a dictionary\n","  \n","  svm_results.append(results_SVM_C(avg_predicted, avg_cluster_Y))\n","    # call the helper method to summarize the svm results\n","  #total_summary(svm_results, test_pd.shape[0],start0,start1,stop0,stop1,start,stop)      \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FPdwEuQw7d6T"},"source":["### run_SVM_baseline"]},{"cell_type":"code","metadata":{"id":"_PtBxwBrx-Xn","colab_type":"code","colab":{}},"source":["@study\n","def run_SVM_baseline(train_pd, test_pd): \n","  \"\"\"\n","  Run SVM+word embedding experiment !\n","  This is the baseline method.\n","  :return:None\n","  \"\"\"\n","  # Create a subplot with 1 row and 2 columns\n","  # print(\"# word2vec:\", word2vec_src)\n","  clf = svm.SVC(kernel=\"rbf\", gamma=0.005)\n","  \n","#   word2vec_model = gensim.models.Word2Vec.load(word2vec_src)\n","#   data = PaperData(word2vec=word2vec_model)\n","#   train_pd = load_vec(data, data.train_data, use_pkl=False)\n","#   test_pd = load_vec(data, data.test_data, use_pkl=False)\n","\n","  train_X = train_pd.loc[:, \"Output\"].tolist()\n","  train_Y = train_pd.loc[:, \"LinkTypeId\"].tolist()\n","  test_X = test_pd.loc[:, \"Output\"].tolist()\n","  test_Y = test_pd.loc[:, \"LinkTypeId\"].tolist()\n","  start = timeit.default_timer()\n","  \n","  #YOUR CODE HERE: START\n","  # Fit the classifier\n","  clf.fit(train_X, train_Y) \n","  #YOUR CODE END.\n","  stop = timeit.default_timer()\n","  \n","  #YOUR CODE HERE: Get the predictions.\n","  predicted = clf.predict(test_X)\n","  print(metrics.classification_report(test_Y, predicted,\n","                                      labels=[\"1\", \"2\", \"3\", \"4\"],\n","                                      digits=3))\n","  cm=metrics.confusion_matrix(test_Y, predicted, labels=[\"1\", \"2\", \"3\", \"4\"])\n","  print(\"accuracy  \", get_acc(cm))\n","  print(\"Model training time: \", stop - start)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e09JM-vfwKg2","colab_type":"text"},"source":["### run_MLP"]},{"cell_type":"code","metadata":{"id":"Ce-0evbQuqtK","colab_type":"code","colab":{}},"source":["\n","@study\n","def run_MLP(train_pd, test_pd): \n","  \"\"\"\n","  Run SVM+word embedding experiment !\n","  This is the baseline method.\n","  :return:None\n","  \"\"\"\n","  # Create a subplot with 1 row and 2 columns\n","  # print(\"# word2vec:\", word2vec_src)\n","  \n","  #<YOUR CODE HERE START...DEFINE AN MLP CLASSIFIER.\n","#   from sklearn.neural_network import MLPClassifier\n","#   mlp = MLPClassifier(max_iter=1000)\n","  \n","#   parameter_space = {\n","#     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n","#     'activation': ['tanh', 'relu'],\n","#     'solver': ['sgd', 'adam'],\n","#     'alpha': [0.0001, 0.05],\n","#     'learning_rate': ['constant','adaptive'],\n","#   }\n","  \n","#   from sklearn.model_selection import GridSearchCV\n","#   clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n","\n","  \n","  from sklearn.neural_network import MLPClassifier\n","  clf = MLPClassifier(\n","      solver='lbfgs', \n","      alpha=1e-4,\n","      hidden_layer_sizes=(50, 50, 50), \n","      random_state=1,\n","      learning_rate_init=0.001,\n","      activation=\"relu\"\n","  )\n","  \n","  # YOUR CODE END\n","  \n","  \n","#   word2vec_model = gensim.models.Word2Vec.load(word2vec_src)\n","#   data = PaperData(word2vec=word2vec_model)\n","#   train_pd = load_vec(data, data.train_data, use_pkl=False)\n","#   test_pd = load_vec(data, data.test_data, use_pkl=False)\n","  \n","  train_X = train_pd.loc[:, \"Output\"].tolist()\n","  train_Y = train_pd.loc[:, \"LinkTypeId\"].tolist()\n","  \n","  test_X = test_pd.loc[:, \"Output\"].tolist()\n","  test_Y = test_pd.loc[:, \"LinkTypeId\"].tolist()\n","  \n","  start = timeit.default_timer()\n","  #YOUR CODE HERE: START\n","  # Fit the classifier\n","  clf.fit(train_X, train_Y)          \n","  #YOUR CODE END.\n","  stop = timeit.default_timer()\n","  \n","  #YOUR CODE HERE: Get the predictions.\n","  predicted = clf.predict(test_X)\n","  print(metrics.classification_report(test_Y, predicted,\n","                                      labels=[\"1\", \"2\", \"3\", \"4\"],\n","                                      digits=3))\n","  cm=metrics.confusion_matrix(test_Y, predicted, labels=[\"1\", \"2\", \"3\", \"4\"])\n","  print(\"accuracy  \", get_acc(cm))\n","  print(\"Model training time: \", stop - start)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CGzIy8PsVKcA","colab_type":"text"},"source":["### run_KNN_baseline"]},{"cell_type":"code","metadata":{"id":"Z5WWBuFLw4TZ","colab_type":"code","colab":{}},"source":["@study\n","def run_KNN_baseline(train_pd, test_pd, k):\n","  \"\"\"\n","  Run KNN+word embedding experiment !\n","  This is the baseline method.\n","  :return:None\n","  \"\"\"\n","  # Create a subplot with 1 row and 2 columns\n","  # print(\"# word2vec:\", word2vec_src)\n","  \n","  # YOUR CODE HERE START..\n","  from sklearn.neighbors import KNeighborsClassifier\n","  clf = KNeighborsClassifier(n_neighbors=k)\n","  # YOUR CODE HERE END\n","  \n","  #word2vec_model = gensim.models.Word2Vec.load(word2vec_src)\n","  #data = PaperData(word2vec=word2vec_model)\n","  #train_pd = load_vec(data, data.train_data, use_pkl=False)\n","  #test_pd = load_vec(data, data.test_data, use_pkl=False)\n","  \n","  train_X = train_pd.loc[:, \"Output\"].tolist()\n","  train_Y = train_pd.loc[:, \"LinkTypeId\"].tolist()\n","  test_X = test_pd.loc[:, \"Output\"].tolist()\n","  test_Y = test_pd.loc[:, \"LinkTypeId\"].tolist()\n","  start = timeit.default_timer()\n","  \n","  #YOUR CODE HERE: START\n","  # Fit the classifier\n","  clf.fit(train_X, train_Y)\n","  #YOUR CODE END.\n","  stop = timeit.default_timer()\n","  \n","  #YOUR CODE HERE: Get the predictions.\n","  predicted = clf.predict(test_X)\n","  print(metrics.classification_report(test_Y, predicted,\n","                                      labels=[\"1\", \"2\", \"3\", \"4\"],\n","                                      digits=3))\n","  cm=metrics.confusion_matrix(test_Y, predicted, labels=[\"1\", \"2\", \"3\", \"4\"])\n","  print(\"accuracy  \", get_acc(cm))\n","  print(\"Model training time: \", stop - start)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u-eDINpHvBP5","colab_type":"text"},"source":["### The code during the first run downloads the word2vec file and hence would wait for about 10 minutes. Subsequent runs would be quicker\n","\n","prepare_word2vec"]},{"cell_type":"code","metadata":{"id":"zxsB47dO7P-U","colab_type":"code","colab":{}},"source":["def prepare_word2vec():\n","  print(\"Downloading pretrained word2vec models\")\n","  url = \"https://zenodo.org/record/807727/files/word2vecs_models.zip\"\n","  file_name = wget.download(url)\n","  with zipfile.ZipFile(file_name, \"r\") as zip_ref:\n","    zip_ref.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"599E_pKY5eUu","colab_type":"code","colab":{}},"source":["word_src = \"word2vecs_models\"\n","threads = []\n","warnings.filterwarnings(\"ignore\")\n","if not os.path.exists(word_src):\n","  prepare_word2vec()\n","elif len(os.listdir(word_src)) == 0:\n","  os.rmdir(word_src)\n","  prepare_word2vec()\n","for x in range(1):\n","  random.seed(x)\n","  np.random.seed(x)#Seeding will help in ensuring that the same order of word2vec files are retrieved below.\n","  #Further multiple models below is used for experimental purposes, but in the case of your hacakthon as you see below, you..\n","  #..are using only the first model. After completion of the mini-hacakthon, you are free to explore with other..\n","  #..word2vec models and discuss with your mentors.\n","  myword2vecs = [os.path.join(word_src, i) for i in os.listdir(word_src)\n","                 if \"syn\" not in i]\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yyXjJSkJhTJK","colab_type":"code","outputId":"d858be09-3f1d-4671-a621-02a8f9b6cd68","executionInfo":{"status":"ok","timestamp":1556963113406,"user_tz":-330,"elapsed":26389,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["print(myword2vecs)\n","print(myword2vecs[x])"],"execution_count":20,"outputs":[{"output_type":"stream","text":["['word2vecs_models/myword2vec463', 'word2vecs_models/myword2vec134', 'word2vecs_models/myword2vec237', 'word2vecs_models/myword2vec226', 'word2vecs_models/myword2vec956', 'word2vecs_models/myword2vec793', 'word2vecs_models/myword2vec844', 'word2vecs_models/myword2vec323', 'word2vecs_models/myword2vec622', 'word2vecs_models/myword2vec236']\n","word2vecs_models/myword2vec463\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mjcWMCRLcjIP","colab_type":"text"},"source":["## Graded problems:"]},{"cell_type":"markdown","metadata":{"id":"QdlGKoXT7IAp","colab_type":"text"},"source":["### Graded 1: Fill the section \"YOUR CODE BELOW\" section within the \"get_document_vec\" function in the beginning of this notebook - 6 Marks\n","The above is necessary in order to proceed with the rest of the sections below. You are basically, loading the word-to-vec representation for your stackoverflow sentences. The feature vector is the average of the two posts in question. "]},{"cell_type":"code","metadata":{"id":"Gw2bIa5eE9UF","colab_type":"code","colab":{}},"source":["word2vec_model = gensim.models.Word2Vec.load(myword2vecs[x])\n","data = PaperData(word2vec=word2vec_model)\n","train_pd = load_vec(data, data.train_data, use_pkl=False)\n","test_pd = load_vec(data, data.test_data, use_pkl=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bgiEg1db5fgl","colab_type":"text"},"source":["### Graded 2: Define appropriate classifiers below (KNN and MLP) using SKLearn, run the cell below. Observe the outputs. Try various parameters and discuss your analysis with the mentors. The weightage here is on the ability to experiment with various parameters and deterimine your observations/conclusion (and not just on classifier definition/call ) = 6* 2 = 12 Marks"]},{"cell_type":"markdown","metadata":{"id":"5tRRFuj1vQOC","colab_type":"text"},"source":["####run_MLP"]},{"cell_type":"code","metadata":{"id":"LfV8i-Oz5Rqa","colab_type":"code","outputId":"34bc66ea-3174-480a-fc18-ca2490ef10d1","executionInfo":{"status":"ok","timestamp":1556963324958,"user_tz":-330,"elapsed":121365,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":432}},"source":["# Go to the function definition above and make the necessary changes and then come back to run this.\n","run_MLP(train_pd, test_pd)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["\n","### run_MLP ##################################################\n","# 2019-05-04 09:48:57\n","# \n","# Run SVM+word embedding experiment !\n","# This is the baseline method.\n","# :return:None\n","# \n","              precision    recall  f1-score   support\n","\n","           1      0.781     0.730     0.755       400\n","           2      0.718     0.720     0.719       400\n","           3      0.947     0.993     0.969       400\n","           4      0.847     0.860     0.854       400\n","\n","   micro avg      0.826     0.826     0.826      1600\n","   macro avg      0.823     0.826     0.824      1600\n","weighted avg      0.823     0.826     0.824      1600\n","\n","accuracy   [0.73, 0.72, 0.9925, 0.86]\n","Model training time:  8.029840980998415\n","\n","------------------------------------------------------------------------\n","# Runtime: 8.047 secs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KIUdU8nVvZzu","colab_type":"text"},"source":["####run_KNN_baseline"]},{"cell_type":"code","metadata":{"id":"0vIzD4G16ISg","colab_type":"code","outputId":"12de53db-ffa5-418b-bb97-f33ca4f4aae9","executionInfo":{"status":"ok","timestamp":1556963332661,"user_tz":-330,"elapsed":127897,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":1261}},"source":["# Go to the function definition above and make the necessary changes to make this function run and then come back to run this.\n","for k in [1,2,3]:\n","  run_KNN_baseline(train_pd, test_pd, k)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["\n","### run_KNN_baseline ##################################################\n","# 2019-05-04 09:49:05\n","# \n","# Run KNN+word embedding experiment !\n","# This is the baseline method.\n","# :return:None\n","# \n","              precision    recall  f1-score   support\n","\n","           1      0.951     0.925     0.938       400\n","           2      0.881     0.963     0.920       400\n","           3      0.973     0.980     0.976       400\n","           4      0.968     0.897     0.931       400\n","\n","   micro avg      0.941     0.941     0.941      1600\n","   macro avg      0.943     0.941     0.941      1600\n","weighted avg      0.943     0.941     0.941      1600\n","\n","accuracy   [0.925, 0.9625, 0.98, 0.8975]\n","Model training time:  0.05474537200279883\n","\n","------------------------------------------------------------------------\n","# Runtime: 0.770 secs\n","\n","### run_KNN_baseline ##################################################\n","# 2019-05-04 09:49:06\n","# \n","# Run KNN+word embedding experiment !\n","# This is the baseline method.\n","# :return:None\n","# \n","              precision    recall  f1-score   support\n","\n","           1      0.754     0.927     0.832       400\n","           2      0.674     0.843     0.749       400\n","           3      0.913     0.887     0.900       400\n","           4      0.973     0.532     0.688       400\n","\n","   micro avg      0.797     0.797     0.797      1600\n","   macro avg      0.828     0.797     0.792      1600\n","weighted avg      0.828     0.797     0.792      1600\n","\n","accuracy   [0.9275, 0.8425, 0.8875, 0.5325]\n","Model training time:  0.03948024700002861\n","\n","------------------------------------------------------------------------\n","# Runtime: 3.308 secs\n","\n","### run_KNN_baseline ##################################################\n","# 2019-05-04 09:49:10\n","# \n","# Run KNN+word embedding experiment !\n","# This is the baseline method.\n","# :return:None\n","# \n","              precision    recall  f1-score   support\n","\n","           1      0.800     0.812     0.806       400\n","           2      0.757     0.865     0.807       400\n","           3      0.897     0.975     0.934       400\n","           4      0.911     0.688     0.783       400\n","\n","   micro avg      0.835     0.835     0.835      1600\n","   macro avg      0.841     0.835     0.833      1600\n","weighted avg      0.841     0.835     0.833      1600\n","\n","accuracy   [0.8125, 0.865, 0.975, 0.6875]\n","Model training time:  0.03926797000167426\n","\n","------------------------------------------------------------------------\n","# Runtime: 3.434 secs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"InXSbde-vccr","colab_type":"text"},"source":["####run_SVM_baseline"]},{"cell_type":"code","metadata":{"id":"OUMzL0tyureN","colab_type":"code","outputId":"795e0459-0de5-4f27-c056-111f3822bba0","executionInfo":{"status":"ok","timestamp":1556963346897,"user_tz":-330,"elapsed":141148,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":432}},"source":["run_SVM_baseline(train_pd, test_pd)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["\n","### run_SVM_baseline ##################################################\n","# 2019-05-04 09:49:13\n","# \n","# Run SVM+word embedding experiment !\n","# This is the baseline method.\n","# :return:None\n","# \n","              precision    recall  f1-score   support\n","\n","           1      0.728     0.535     0.617       400\n","           2      0.526     0.497     0.512       400\n","           3      0.787     0.970     0.869       400\n","           4      0.595     0.647     0.620       400\n","\n","   micro avg      0.662     0.662     0.662      1600\n","   macro avg      0.659     0.662     0.654      1600\n","weighted avg      0.659     0.662     0.654      1600\n","\n","accuracy   [0.535, 0.4975, 0.97, 0.6475]\n","Model training time:  12.035810646000755\n","\n","------------------------------------------------------------------------\n","# Runtime: 14.252 secs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x2nvIZaV6dUo","colab_type":"text"},"source":["### Graded 3: Define a Kmeans using Sklearn; The objective of clustering is to enable classifer to act at the cluster level. The expectation here is your ability to appreciate the clustering and experiment with the same -  7 Marks\n","You just need to define and experiment with SKlearn's Kmeans with various parameters (especially number of clusters which is providing you the right outout) and discuss the output with the mentors.</br>\n","** NOTE : This clustering takes time unlike the above code. So please be sure of your code before deploying, else, you'll end up spending a lot of time waiting **"]},{"cell_type":"code","metadata":{"id":"DoybOMDI6Q9Q","colab_type":"code","outputId":"b9afa608-e1f5-4c3d-b911-c6db0ed9af06","executionInfo":{"status":"ok","timestamp":1556963316363,"user_tz":-330,"elapsed":229301,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":1624}},"source":[" # Go to the function definition above and make the necessary changes to make this function run and then come back to run this.\n","\n","run_kmeans_m(train_pd, test_pd)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Inter -  0.0009482780005782843\n","Thread No 0\n","Thread No 1\n","Thread No 2\n","Thread No 3\n","\n","### run_tuning_SVM_C ##################################################\n","# 2019-05-04 09:46:40\n","### run_tuning_SVM_C\n","\n","###\n","### #  ################################################## run_tuning_SVM_C\n","\n","# :param word2vec_src:str, path of word2vec model\n","# :param repeats:int, number of repeats\n","# :param fold: int,number of folds\n","# :param tuning: boolean, tuning or not.\n","# :return: None\n","# \n","#run_tuning_SVM_C  2019-05-04 09:46:40##################################################\n","\n","#  #\n","# :param word2vec_src:str, path of word2vec model\n","# :param repeats:int, number of repeats\n","# :param fold: int,number of folds\n","# :param tuning: boolean, tuning or not.\n","# :return: None\n","# ################################################## \n","\n","2019-05-04 09:46:40\n","##  2019-05-04 09:46:40\n","# \n","# :param word2vec_src:str, path of word2vec model\n","# :param repeats:int, number of repeats\n","# :param fold: int,number of folds\n","# :param tuning: boolean, tuning or not.\n","# :return: None\n","# \n","\n","# :param word2vec_src:str, path of word2vec model\n","# :param repeats:int, number of repeats\n","# :param fold: int,number of folds\n","# :param tuning: boolean, tuning or not.\n","# :return: None\n","# \n","\n","------------------------------------------------------------------------\n","# Runtime: 48.460 secs\n","\n","------------------------------------------------------------------------\n","# Runtime: 77.228 secs\n","\n","------------------------------------------------------------------------\n","# Runtime: 131.967 secs\n","\n","------------------------------------------------------------------------\n","# Runtime: 136.606 secs\n","len(svm_models[l])-1= 2\n","numClusters= 4\n","              precision    recall  f1-score   support\n","\n","           1      0.780     0.850     0.813       400\n","           2      0.816     0.752     0.783       400\n","           3      0.949     0.978     0.963       400\n","           4      0.841     0.805     0.822       400\n","\n","   micro avg      0.846     0.846     0.846      1600\n","   macro avg      0.846     0.846     0.845      1600\n","weighted avg      0.846     0.846     0.845      1600\n","\n","numClusters= 4\n","              precision    recall  f1-score   support\n","\n","           1      0.840     0.787     0.813       400\n","           2      0.810     0.780     0.795       400\n","           3      0.953     0.973     0.963       400\n","           4      0.817     0.882     0.849       400\n","\n","   micro avg      0.856     0.856     0.856      1600\n","   macro avg      0.855     0.856     0.855      1600\n","weighted avg      0.855     0.856     0.855      1600\n","\n","              precision    recall  f1-score   support\n","\n","           1      0.808     0.819     0.813       800\n","           2      0.813     0.766     0.789       800\n","           3      0.951     0.975     0.963       800\n","           4      0.828     0.844     0.836       800\n","\n","   micro avg      0.851     0.851     0.851      3200\n","   macro avg      0.850     0.851     0.850      3200\n","weighted avg      0.850     0.851     0.850      3200\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AxKpDLP8MwNV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}