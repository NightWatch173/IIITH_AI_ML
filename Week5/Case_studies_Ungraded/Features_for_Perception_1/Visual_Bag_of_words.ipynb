{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Visual_Bag_of_words.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"fG2XKTW1TvGC","colab_type":"text"},"cell_type":"markdown","source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"metadata":{"id":"mj8VIyj8TylG","colab_type":"text"},"cell_type":"markdown","source":["### Not for Grading"]},{"metadata":{"id":"7ae5aVdIQ8fR","colab_type":"text"},"cell_type":"markdown","source":["Visual Bag of words is commonly used in image classification. Its concept is adapted from information retrieval and NLP’s bag of words (BoW). In bag of words (BoW), we count the number of each word appears in a document, use the frequency of each word to know the keywords of the document, and make a frequency histogram from it. We treat a document as a bag of words (BoW). We have the same concept in bag of Visual Bag of words, but instead of words, we use image features as the “words”. Image features are unique pattern that we can find in an image. The objective of this experiment is to visualize the implementation of Visual Bag of words"]},{"metadata":{"id":"-sc3kdc_RTzX","colab_type":"text"},"cell_type":"markdown","source":["##Basic Pipe line of this case study\n","###Image -> Feature Extraction -> Dictionary Building -> Coding -> Pooling -> Classification model"]},{"metadata":{"id":"ZHUTCChDUKcN","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"NCSgzwt6UFgf","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P18_test\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SEPVBHRzUFkz","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"912345678\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uPG3-V87UF1l","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook=\"M1W2_CS_11_Visual_Bag_of_words\" #name of the notebook\n","Answer = \"Ungraded\"\n","def setup():\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Casestudies_slides/Visual_BoW/Images.zip\")\n","    ipython.magic(\"sx unzip Images.zip\")\n","    ipython.magic(\"sx pip install opencv-python==3.4.2.16\")\n","    ipython.magic(\"sx pip install opencv-contrib-python==3.4.2.16\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", r[\"date\"])\n","      print(\"Time of submission: \", r[\"time\"])\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fe6hHTp6gAs-","colab_type":"text"},"cell_type":"markdown","source":["##*Please restart the runtime after running the set up block and continue from the next cell*"]},{"metadata":{"id":"XRu7SU-CU1I0","colab_type":"text"},"cell_type":"markdown","source":["### Importing required packages"]},{"metadata":{"id":"dUVbEMkzT4fJ","colab_type":"code","colab":{}},"cell_type":"code","source":["import cv2\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TSAnu6yCH7yp","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.cluster import KMeans\n","from sklearn import svm\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import svm, datasets\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y85t5FClH7yx","colab_type":"text"},"cell_type":"markdown","source":["## Training Images: "]},{"metadata":{"id":"_XZ3dpMyH7yz","colab_type":"text"},"cell_type":"markdown","source":["#### Given category name, dictionary to which we need to add images, index and whether images are test/train type returns dictionary with adding images present in the category.\n","\n","#### Parameters: category - category name.\n","image_dict - dictionary to which images should be stored.\n","index - key value for dictionary.\n","i_type - either \"test\"/\"train\"/\"kmeans\"\n","#### Returns:\n","Nothing, since image_dict is already modified above."]},{"metadata":{"id":"muZfdE_OH7y0","colab_type":"code","colab":{}},"cell_type":"code","source":["def add_images(category, image_dict, index, i_type = \"test\"):\n","    temp_path = os.path.join(i_type, category)\n","    pathname = os.path.join(\"./Images\", temp_path)\n","    image_dict[index] = []\n","    print(\"Category: \" + str(category))\n","    for img in os.listdir(pathname):\n","        path_temp = os.path.join(pathname,img)\n","        print(path_temp)\n","        temp_img = cv2.imread(path_temp)\n","        image_dict[index].append(temp_img)\n","    return "],"execution_count":0,"outputs":[]},{"metadata":{"id":"CakrY5VcH7y5","colab_type":"text"},"cell_type":"markdown","source":["\n","#### Given categories and whether test/train type images, searches in the images directory accordingly and returns the images.\n","\n","#### Parameters: \n","    * categories - list of strings. (Each string is a category).\n","    * i_type = specifies which images(i.e test/train)\n","    * Default is \"test\"\n","\n","#### Return: \n","        returns a dictionary, with keys representing \n","        categories and its values representing images \n","        present in the category."]},{"metadata":{"id":"rnVRbXHIH7y7","colab_type":"text"},"cell_type":"markdown","source":["#### Reading image files:"]},{"metadata":{"id":"fxaL-E4LH7y8","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_image_dict(categories, i_type = \"test\"):\n","\n","    if i_type == \"test\" or i_type == \"train\" or i_type == \"kmeans\":\n","        # Run only if i_type is either test/train.\n","        image_dict = {}\n","        for (index, category) in list(enumerate(categories)):\n","            add_images(category, image_dict, index, i_type)\n","        return image_dict\n","    else:\n","        raise Error(\"image type is not test or train\")\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rePn5aEzH7zB","colab_type":"code","colab":{}},"cell_type":"code","source":["categories = ['airplanes', 'chandelier', 'motorbikes', 'butterfly', 'revolver', 'spoon']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aOWuH6OaH7zF","colab_type":"code","colab":{}},"cell_type":"code","source":["train_dict = get_image_dict(categories, \"train\")\n","test_dict =  get_image_dict(categories, \"test\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J1oBv2RSH7zO","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.imshow(train_dict[0][0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sss0RtCKH7zT","colab_type":"text"},"cell_type":"markdown","source":["## Parsing image files into categories:"]},{"metadata":{"id":"WNVS7XeYH7zV","colab_type":"text"},"cell_type":"markdown","source":["#### Given an img_dict, creates two lists img_labels and img_list\n","\n","Generally img_dict is modelled as to contain label as key\n","and the key's value contains a list which has all the \n","images present in the category.\n","\n","#### Parameters:\n","            img_dict - dictionary with keys containing label category\n","                       and corresponding values containing images\n","                       in that category.\n","\n","#### Returns:\n","            (img_list, img_labels)\n","            img_labels: contains labels of all the images.\n","            img_list: contains list of all images."]},{"metadata":{"id":"prbxv65EH7zX","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_images_and_labels(img_dict):\n","    img_labels = []\n","    img_list = []\n","    for key in img_dict.keys():\n","            for img in img_dict[key]:                \n","                img_labels.append(key)\n","                img_list.append(img)\n","    return (img_list,  img_labels)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o6aFvyq1H7zb","colab_type":"code","colab":{}},"cell_type":"code","source":["[train_imgs, train_labels] = get_images_and_labels(train_dict)\n","[test_imgs, test_labels] = get_images_and_labels(test_dict)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ubEfk6miH7zf","colab_type":"text"},"cell_type":"markdown","source":["## Feature Extraction"]},{"metadata":{"id":"qt2S8ITpZdsv","colab_type":"text"},"cell_type":"markdown","source":["##SIFT \n","\n","SIFT is a technique for detecting salient, stable feature points in an image. \n","\n","For every such point, it also provides a set of **“features”** that **“characterize/describe”** a\n","small image region around the point. \n","\n","These features are **invariant to rotation and scale.**\n","\n","###Basic steps of SIFT algorithm: (Overview)\n","* Determine approximate location and scale of salient feature points (also called keypoints)\n","* Refine their location and scale\n","* Determine orientation(s) for each keypoint.\n","* Determine descriptors for each keypoint.\n","\n"]},{"metadata":{"id":"XgxHkWAxZfEn","colab_type":"text"},"cell_type":"markdown","source":["The steps to get a 128 D SIFT  descriptors for each key point:\n","\n","* Region rescaled to a grid of 16x16 pixels.\n","\n","* 4x4 regions = 16 histograms (concatenated).\n","\n","* Histograms: 8 orientation bins, gradients weighted by gradient magnitude.\n","\n","* Final descriptor has 128 dimensions and is normalized to compensate for illumination differences"]},{"metadata":{"id":"T9f8U8zQZiuQ","colab_type":"text"},"cell_type":"markdown","source":["More details on SIFT can be found [here.](https://courses.cs.washington.edu/courses/cse576/06sp/notes/Interest2.pdf)"]},{"metadata":{"id":"71dwlbNGH7zh","colab_type":"text"},"cell_type":"markdown","source":["#### Given a list of images, it generates of vocabulary which contains descriptors of all the images present in the img_list.\n","\n","#### Parameters:\n","\n","img_list - list containing all the images.\n","\n","#### Returns:\n","\n","(vocab_list, temp_list)\n","temp_list - list, where each element of the list is a numpy array containing all the feature descriptors of the patches present in an image.\n","\n","#### For example, \n","[1].shape == (x, 128)\n","\n","vocab_list: a numpy array, which contains all the \n","descriptors present in all the images.\n","\n","vocab_list.shape == (x, 128)\n","\n","where x is the total no of descriptors\n","present in the images.\n","\n","128- size of descriptor.   "]},{"metadata":{"id":"129IHHKWH7zi","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure(figsize=(15,15))\n","plt.imshow(train_imgs[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UAhhzpv5H7zo","colab_type":"code","colab":{}},"cell_type":"code","source":["gray = cv2.cvtColor(train_imgs[0], cv2.COLOR_BGR2GRAY) #Changing the training images to grayscale\n","sift = cv2.xfeatures2d.SIFT_create() #Defining SIFT operator\n","kp, des = sift.detectAndCompute(gray, None) #Applying SIFT operator on the training images\n","#Getting keypoints from SIFT applied one of the training images. Here, 0th image of training images\n","img=0\n","kp_img = cv2.drawKeypoints(gray,kp,img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"iN67A8BKX1Ks","colab_type":"text"},"cell_type":"markdown","source":["###Visualizing one of the the SIFT applied training images"]},{"metadata":{"id":"yM6fXEHIH7zs","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure(figsize=(15,15))\n","plt.imshow(kp_img)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-ls5wIOwZwuS","colab_type":"text"},"cell_type":"markdown","source":["###Defining the function for getting cluster centers of the image"]},{"metadata":{"id":"bMqjw6WgH7zy","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_cluster_centres(jpg_img, k = 8):\n","    img = jpg_img\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    sift = cv2.xfeatures2d.SIFT_create()\n","    kp, des = sift.detectAndCompute(gray, None)\n","    [num, size] = des.shape\n","    kmeans = KMeans(k)\n","    ret = kmeans.fit_predict(des)\n","    C = kmeans.cluster_centers_\n","    return C"],"execution_count":0,"outputs":[]},{"metadata":{"id":"STSdwG8TH7z2","colab_type":"code","colab":{}},"cell_type":"code","source":["get_cluster_centres(train_imgs[0], ).shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I9uUkctOZ7xb","colab_type":"text"},"cell_type":"markdown","source":["###Defining the function for getting SIFT feature descriptors of the image"]},{"metadata":{"id":"XkE-0JgMH70C","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_sift_feature_descriptors(jpg_img):\n","    img = jpg_img\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    sift = cv2.xfeatures2d.SIFT_create()\n","    kp, des = sift.detectAndCompute(gray, None)\n","    return des"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jY7fFVeyaGnG","colab_type":"text"},"cell_type":"markdown","source":["###Defining the function for generating vocabulary of the given image list"]},{"metadata":{"id":"qtlfgZ2sH70N","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_vocabulary(img_list):\n","    temp_list = []\n","    #flag = True\n","    for img in img_list:\n","        features = get_sift_feature_descriptors(img)\n","        temp_list.append(features)\n","\n","    vocab_list = temp_list[0]\n","\n","    for ftr in range(1, len(temp_list)):\n","        vocab_list = np.vstack((vocab_list, temp_list[ftr]))\n","        \n","    vocabulary = vocab_list\n","    return (vocab_list, temp_list)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gtNiUmd3H70T","colab_type":"text"},"cell_type":"markdown","source":["## Dictionary Building:"]},{"metadata":{"id":"V8VuG3qAZ4dC","colab_type":"text"},"cell_type":"markdown","source":["#### Given vocabulary, and number of clusters in which vocabulary is to be divided, applies kmeans algorithm and returns \n","\n","#### Parameters:\n","                    vocab - vocabulary containing feature descriptors\n","                    from all the images available for training.\n","\n","                    n_clusters - Number of clusters for dividing vocab.\n","#### Returns:\n","                    kmeans - which is further used to predict."]},{"metadata":{"id":"pEvhhIfrH70V","colab_type":"code","colab":{}},"cell_type":"code","source":["(f_vocabulary, i_vocab)= generate_vocabulary(train_imgs)\n","(t_f_vocabulary, t_i_vocab)= generate_vocabulary(test_imgs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xiLev3ByH70Z","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.imshow(f_vocabulary[10].reshape(16,8))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OSXiafvgbozN","colab_type":"text"},"cell_type":"markdown","source":["###Defining a function to generate clusters"]},{"metadata":{"id":"jGUkqRzEH70g","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_clusters(vocab, n_clusters = 500):\n","    kmeans = KMeans(n_clusters).fit(vocab)\n","    return kmeans"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vQWqW8pjH70m","colab_type":"code","colab":{}},"cell_type":"code","source":["n_clusters = 1000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Rhvd6HxTH70q","colab_type":"text"},"cell_type":"markdown","source":["## Coding:"]},{"metadata":{"id":"8de3F9eQH70q","colab_type":"text"},"cell_type":"markdown","source":["`** Note: This will take quite some time **"]},{"metadata":{"id":"FgktXNeSH70s","colab_type":"code","colab":{}},"cell_type":"code","source":["kmeans = generate_clusters(f_vocabulary, n_clusters)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ITEGyWMZH70x","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_features(temp_list, kmeans, n_clusters):\n","    \"\"\"\n","        given cluster centres(via kmeans) which are obtained from\n","        all the training images, what this function does is,\n","        for each image it creates a histogram where x axis is \n","        nothing but all the clusters(0 to n_clusters) and\n","        for each descriptor of the image we calculate to which\n","        cluster it belongs and for each cluster y-axis contains\n","        no of times a cluster is predicted via kmeans.predict(descriptor).\n","\n","        This can be further used for training classifier.\n","\n","        Parameters:\n","                    temp_list = is the same temp_list from \n","                                generate_vocabulary.\n","                    n_clusters = no of clusters in which training \n","                                vocab is divided into.\n","                    kmeans = kmeans obtained from generate_clusters.\n","\n","        Returns:\n","                    p_array - a numpy array which is of size\n","                              no_images x no_of_clusters.\n","    \"\"\"\n","                    \n","    t_length = len(temp_list)\n","    p_array = np.zeros((1,n_clusters))\n","\n","    for i in range(0, t_length):\n","\n","        features = temp_list[i]\n","        (x1, y1) = features.shape\n","\n","        t_array = np.zeros((1, n_clusters)) #an array for each image\n","\n","        for j in range(0, x1):\n","            des = features[j]\n","            des = des.reshape(1, 128)\n","            ind = kmeans.predict(des)\n","            t_array[0][ind] += 1\n","\n","        p_array = np.vstack((p_array, t_array))\n","    \n","    p_array = p_array[1:]\n","    return p_array"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dQYKyTNmH702","colab_type":"text"},"cell_type":"markdown","source":["## Pooling"]},{"metadata":{"id":"bKBOy5YkH703","colab_type":"code","colab":{}},"cell_type":"code","source":["train_data = generate_features(i_vocab, kmeans, n_clusters)\n","test_data =  generate_features(t_i_vocab, kmeans, n_clusters)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hfG_CzRVH707","colab_type":"text"},"cell_type":"markdown","source":["## Classification"]},{"metadata":{"scrolled":true,"id":"StAB4FTLH709","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"Applying SVM classifier.\")\n","# SVM Classifier.\n","clf = svm.SVC()\n","fitted = clf.fit(train_data, train_labels)\n","predict = clf.predict(test_data)"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"ut4GllLmH71C","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"Actual:\")\n","print(test_labels)\n","print(\"predicted:\")\n","print(predict)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4vaonXe7qG93","colab_type":"text"},"cell_type":"markdown","source":["##Calculating Confusion matrix"]},{"metadata":{"id":"C8OpZXigH71G","colab_type":"code","colab":{}},"cell_type":"code","source":["#Confusion matrix.\n","test_labels = np.asarray(test_labels)\n","cnf_matrix = confusion_matrix(predict, test_labels)\n","np.set_printoptions(precision = 2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7T0HXdrlqNkd","colab_type":"text"},"cell_type":"markdown","source":["##Visualizing Confusion matrix"]},{"metadata":{"id":"mdW63GY4H71J","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VUk6xijwH71M","colab_type":"code","colab":{}},"cell_type":"code","source":["plot_confusion_matrix(cnf_matrix, classes = categories,\n","                                title='Confusion matrix')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BdWhN6_0kmZ4","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"metadata":{"id":"TzPPfeG4km5K","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Good, But Not Challenging for me\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"74U4EaWBkqu_","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"test'\\\"\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9HFWek1Pkq7t","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"Yes\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w-Rq5kN4krGo","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Run this cell to submit your notebook  { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":0,"outputs":[]}]}