{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Backpropagation_Ungraded.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"pNDAGCxqiu1a","colab_type":"text"},"cell_type":"markdown","source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n"]},{"metadata":{"id":"DESjVL0v_aD9","colab_type":"text"},"cell_type":"markdown","source":["##Learning Objectives"]},{"metadata":{"id":"tTyuzqDcOfOi","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","At the end of the experiment, you will be able to:\n","\n","* Understand the challenges of handwriting recognition\n","\n","* Understand the MNIST dataset and classify the MNIST dataset using MLP and back propagation techniques\n","\n","* Understand the feedforward and backpropagation implementation"]},{"metadata":{"id":"Wjc3ME8q_iCF","colab_type":"text"},"cell_type":"markdown","source":["##Dataset \n"]},{"metadata":{"id":"RmGayWV6XhuM","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","###Description\n","\n","\n","1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples, \n","which means each digit occurs 6000 times in the training set and 1000 times in the testing set. \n","2. Each image is Size Normalized and Centered \n","3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value. \n","4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n","\n","### History\n","\n","Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90â€™s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n","\n","Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license. \n","\n","It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students.\n","\n","\n","\n","###Challenges\n","\n","Now, if you notice the images below, you will find that between 2 characters there are always certain similarities and differences. To teach a machine to recognize these patterns and identify the correct output.\n","\n","![altxt](https://www.researchgate.net/profile/Radu_Tudor_Ionescu/publication/282924675/figure/fig3/AS:319968869666820@1453297931093/A-random-sample-of-6-handwritten-digits-from-the-MNIST-data-set-before-and-after.png)\n","\n","Hence, all these challenges make this a good problem to solve in Machine Learning."]},{"metadata":{"id":"2VPM-89u_mrR","colab_type":"text"},"cell_type":"markdown","source":["##Domain Information"]},{"metadata":{"id":"YUxjMHM9liDw","colab_type":"text"},"cell_type":"markdown","source":["\n","Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n","\n","![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n","\n","The experiment handles a subset of text recognition, namely recognizing the 10 numerals (0 to 9) from scanned images.\n"]},{"metadata":{"id":"BjpPB97fE0nH","colab_type":"text"},"cell_type":"markdown","source":["## Block Diagram \n"]},{"metadata":{"id":"d0Ilc3jqq84H","colab_type":"text"},"cell_type":"markdown","source":["\n","The overview of the experiment, i.e. flow of the experiment is explained  through a below block diagram.\n","\n","![alttxt]( https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/flow chart.png)\n","\n"]},{"metadata":{"id":"ynpfQ_sIYmQ8","colab_type":"text"},"cell_type":"markdown","source":["##AI/ML Technique\n"]},{"metadata":{"id":"wx-g0e1Giu1f","colab_type":"text"},"cell_type":"markdown","source":["### What is  MLP ?\n"]},{"metadata":{"id":"IiVNpulviu1g","colab_type":"text"},"cell_type":"markdown","source":["A multilayer perceptron is a class of feedforward artificial neural network. An MLP consists of, at least, three layers of nodes as shown in below image: \n","\n","**Layer1** :   Input Layer\n","\n","**Layer 2** :  Hidden Layer\n","\n","**Layer 3** : Output Layer\n","\n","![alt text](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n","\n","The number of nodes in the input layer is determined by the dimensionality of our data. \n","\n","The number of nodes in the output layer is determined by the number of classes we have.\n"]},{"metadata":{"id":"mLxDcpqeiu1h","colab_type":"text"},"cell_type":"markdown","source":[" ### Making predictions using Feedforward Propagation\n","\n"]},{"metadata":{"id":"trbEjRajiu1i","colab_type":"text"},"cell_type":"markdown","source":["Our network makes predictions using *forward propagation*, which is just a bunch of matrix multiplications and the application of the activation function(s) which we defined above. If $x$ is the $N$-dimensional input to our network then we calculate our prediction $\\hat{y}$ (of lets say dimension $C$) as mentioned below:\n","\n","![alttxt]( https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/1_b.png)\n","\n","$$\n","\\begin{aligned}\n","z_1 & = xW_1 + b_1 \\\\\n","a_1 & = \\tanh(z_1) \\\\\n","z_2 & = a_1W_2 + b_2 \\\\\n","a_2 & = \\hat{y} = \\mathrm{softmax}(z_2)\n","\\end{aligned}\n","$$\n","\n","$z_i$ is the weighted sum of inputs of layer $i$ (bias included) and $a_i$ is the output of layer $i$ after applying the activation function. $W_1, b_1, W_2, b_2$ are  parameters of our network, which we learn from our training data. You can think of them as matrices transforming data between layers of the network. \n","\n","Looking at the above matrix multiplications we can figure out the dimensionality of these matrices. If we use 100 nodes for our hidden layer then $W_1 \\in \\mathbb{R}^{N\\times100}$, $b_1 \\in \\mathbb{R}^{100}$, $W_2 \\in \\mathbb{R}^{100\\times C}$, $b_2 \\in \\mathbb{R}^{C}$ . "]},{"metadata":{"id":"hMJW49Ywiu1j","colab_type":"text"},"cell_type":"markdown","source":["### Using Backpropogation "]},{"metadata":{"id":"Mw-Tg2UYiu1l","colab_type":"text"},"cell_type":"markdown","source":["Learning the parameters for our network means that we should find parameters ($W_1, b_1, W_2, b_2$) that minimizes the error on our training data. But how do we define error? We call the function that measures our error the *loss function*. A common choice with the softmax output is the cross-entropy loss. If we have $N$ training examples and $C$ classes then the loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n","\n","$$\n","\\begin{aligned}\n","L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n","\\end{aligned}\n","$$\n","\n","The formula looks complicated, but all it really does is sum over our training examples and adds to the loss if we predict the incorrect class. So, the further away $y$ (the correct labels) and $\\hat{y}$ (our predictions) are, the greater our loss will be. \n","\n","Remember that our goal is to find the parameters that minimize our loss function. We use gradient descent to find its minimum. Here, we implement the most vanilla version of gradient descent, also called batch gradient descent with a fixed learning rate. Variations such as SGD (stochastic gradient descent) or minibatch gradient descent typically perform better in practice but we are not applying  that in this experiment.\n","\n","As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. To calculate these gradients we use the famous *back propagation algorithm*, which efficiently calculates the gradient starting from the output.\n","\n","By applying the back propagation formula using chain rule we find the following:\n","![alttxt]( https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/2_b.png)\n","\n","$$\n","\\begin{aligned}\n","& \\delta_3 = \\frac{\\partial{L}}{\\partial{z_2}} = \\frac{\\partial{L}}{\\partial{a_2}}\\times\\frac{\\partial{a_2}}{\\partial{z_2}} = -(y - \\hat{y})\\\\\n","\\end{aligned}\n","$$\n","where $a_2$ is $\\hat{y}$\n","$$\n","\\begin{aligned}\n","& \\frac{\\partial{L}}{\\partial{W_2}} = \\frac{\\partial{L}}{\\partial{z_2}}\\times\\frac{\\partial{z_2}}{\\partial{W_2}} = a_1^T \\delta_3  \\\\\n","& \\frac{\\partial{L}}{\\partial{b_2}} = \\frac{\\partial{L}}{\\partial{z_2}}\\times\\frac{\\partial{z_2}}{\\partial{b_2}} = \\delta_3\\\\\n","& \\delta_2 = \\frac{\\partial{L}}{\\partial{z_1}} = \\frac{\\partial{L}}{\\partial{z_2}}\\times\\frac{\\partial{z_2}}{\\partial{a_1}}\\times\\frac{\\partial{a_1}}{\\partial{z_1}} = (1 - \\tanh^2z_1) \\circ \\delta_3W_2^T \\\\\n","& \\frac{\\partial{L}}{\\partial{W_1}} = \\frac{\\partial{L}}{\\partial{z_1}}\\times\\frac{\\partial{z_1}}{\\partial{W_1}} = x^T \\delta_2\\\\\n","& \\frac{\\partial{L}}{\\partial{b_1}} = \\frac{\\partial{L}}{\\partial{z_1}}\\times\\frac{\\partial{z_1}}{\\partial{b_1}} = \\delta_2 \\\\\n","\\end{aligned}\n","$$\n","\n","$\\delta_3 = $ derivative of cross-entropy loss with Softmax as Activation [We will not go into its derivation]\n"]},{"metadata":{"id":"_MlnfaBk_1la","colab_type":"text"},"cell_type":"markdown","source":["##Keywords\n"]},{"metadata":{"id":"w72pzh4dYH2a","colab_type":"text"},"cell_type":"markdown","source":["\n","Multilayer Perceptron (MLP)\n","\n"," BackPropagation\n","\n"," Chain Rule\n","\n"," Softmax\n","\n"," Activation Function\n","\n"," Gradient Descent"]},{"metadata":{"id":"DIsoG2r8Zb0W","colab_type":"text"},"cell_type":"markdown","source":["## Expected time to complete the experiment is : 60min"]},{"metadata":{"id":"D45Og15HkFdC","colab_type":"text"},"cell_type":"markdown","source":["### Setup Steps"]},{"metadata":{"id":"HgrjC9OQkHNl","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"P181902118\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nJK_oO17kJKz","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"8860303743\" #@param {type:\"string\"}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"557AtS1UOd_K","colab_type":"code","cellView":"form","outputId":"8570aa64-9b22-41f8-f58a-84476f60d8f1","executionInfo":{"status":"ok","timestamp":1556353986725,"user_tz":-330,"elapsed":1793,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"Backpropagation_Backup\" #name of the notebook\n","Answer = 'UNGRADED'\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","   \n","   print (\"Setup completed successfully\")\n","   return\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      print(\"Your submission is successful.\")\n","      print(\"Ref Id:\", submission_id)\n","      print(\"Date of submission: \", r[\"date\"])\n","      print(\"Time of submission: \", r[\"time\"])\n","      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if Additional: return Additional      \n","    else: raise NameError('')\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","  \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"metadata":{"id":"jQzrNGJjiu1m","colab_type":"text"},"cell_type":"markdown","source":["We will perform the experiment in 3 steps :"]},{"metadata":{"id":"xPr3RBukiu1n","colab_type":"text"},"cell_type":"markdown","source":["#### 1. Loading the dataset"]},{"metadata":{"id":"itdGOLOgiu1o","colab_type":"code","colab":{}},"cell_type":"code","source":["# Importing Required Packages\n","import numpy as np\n","from scipy import ndimage \n","from matplotlib import pyplot as plt\n","from sklearn import manifold, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import Perceptron\n","from sklearn.metrics import accuracy_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5eKvBMUAP_67","colab_type":"text"},"cell_type":"markdown","source":["####  Now we will load the dataset from sklearn datasets package"]},{"metadata":{"id":"rMKiT9zpiu1u","colab_type":"code","outputId":"bb7eb439-a7cc-4bc9-994d-256bc535b253","executionInfo":{"status":"ok","timestamp":1556354001911,"user_tz":-330,"elapsed":761,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"cell_type":"code","source":["#Load MNIST dataset \n","digits = datasets.load_digits(n_class=10)\n","# Storing features in 'X' variable\n","X = digits.data\n","# Storing labels in 'Y' variable\n","Y = digits.target\n","# Checking the shape of variable 'X' and 'Y'\n","print(X.shape, Y.shape)\n","# Storing number of samples\n","num_examples = X.shape[0]  ## training set size\n","## input layer dimensionality\n","nn_input_dim = X.shape[1]       \n","print(nn_input_dim)\n","## output layer dimensionality\n","nn_output_dim = len(np.unique(Y))    \n","print(nn_output_dim)\n","# Defining the parameters\n","params = {\n","    \"lr\":1e-5,        ## learning_rate\n","    \"max_iter\":1000,\n","    \"h_dimn\":40,     ## hidden_layer_size\n","}"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(1797, 64) (1797,)\n","64\n","10\n"],"name":"stdout"}]},{"metadata":{"id":"T9TiAV9diu11","colab_type":"text"},"cell_type":"markdown","source":["#### 2. Writing helper functions for the MLP"]},{"metadata":{"id":"vF49f_Zxghd8","colab_type":"text"},"cell_type":"markdown","source":["Let us define the skeleton of the model which is a 3 layer neural network with one input layer, one hidden layer and one output layer."]},{"metadata":{"id":"k0FKcj0vbFPN","colab_type":"code","colab":{}},"cell_type":"code","source":["def build_model():\n","    hdim = params[\"h_dimn\"]\n","    # Initialize the parameters to random values.\n","    np.random.seed(0)\n","    # here nn_input_dim is nothing but the number of features, hdim is the dimension of hidden layer.\n","    # So the total nbr of weights for first layer (input to hidden) would have dimension nn_input_dim*hdim\n","    # Note: We also normalize the weights so that lie within a standard range (defined by sq-root of nbr of input dimensions)\n","    W1 = np.random.randn(nn_input_dim, hdim) / np.sqrt(nn_input_dim)\n","    b1 = np.zeros((1, hdim))\n","    # The nbr of 2nd layer i.e. hidden to output \n","    W2 = np.random.randn(hdim, nn_output_dim) / np.sqrt(hdim)\n","    b2 = np.zeros((1, nn_output_dim))\n","\n","    # This is what we return at the end\n","    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-QPM03Pbb3og","colab_type":"text"},"cell_type":"markdown","source":["**Softmax**\n","\n","The softmax function is often used in the final layer of a neural network-based classifier. The main advantage of using Softmax is that it outputs probability. The range of the probability will be from 0 to 1, and the sum of all the probabilities in the output layer will be equal to one. If the softmax function used for multi-classification model it returns the probabilities of each class and the target class is the one with the highest probability.\n","\n","Now let us define a function to calculate the softmax value:"]},{"metadata":{"id":"XTDeMu7GbIHs","colab_type":"code","colab":{}},"cell_type":"code","source":["def softmax(x):\n","    exp_scores = np.exp(x)\n","    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n","    return probs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YeJD44HPh4Uy","colab_type":"text"},"cell_type":"markdown","source":["Let us define a function for forward propagation."]},{"metadata":{"id":"e3LTvG_bbLPx","colab_type":"code","colab":{}},"cell_type":"code","source":["def feedforward(model, x):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    z1 = x.dot(W1) + b1\n","    a1 = np.tanh(z1)#This is the non-linearity applied to the output\n","    z2 = a1.dot(W2) + b2\n","    probs = softmax(z2)\n","    return a1, probs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uOyUPft5iBOw","colab_type":"text"},"cell_type":"markdown","source":["Let us define a function for backpropagation"]},{"metadata":{"id":"FkhZf5SjbN8R","colab_type":"code","colab":{}},"cell_type":"code","source":["def backpropagation(model, x, y, a1, probs):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    \n","    delta3 = probs\n","    delta3[range(y.shape[0]), y] -= 1\n","    dW2 = (a1.T).dot(delta3)\n","    db2 = np.sum(delta3, axis=0, keepdims=True)\n","    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n","    dW1 = np.dot(x.T, delta2)\n","    db1 = np.sum(delta2, axis=0)\n","    return dW2, db2, dW1, db1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jQny-gqTeGGl","colab_type":"text"},"cell_type":"markdown","source":["Now let us write a function to calculate loss."]},{"metadata":{"id":"ap2eV_qYbQ0G","colab_type":"code","colab":{}},"cell_type":"code","source":["def calculate_loss(model, x, y):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","   \n","    # Forward propagation to calculate predictions\n","    _, probs = feedforward(model, x)\n","    \n","    # Calculating the cross entropy loss\n","    corect_logprobs = -np.log(probs[range(y.shape[0]), y])\n","    data_loss = np.sum(corect_logprobs)\n","    \n","    return 1./y.shape[0] * data_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iUx9rDdOc6sm","colab_type":"text"},"cell_type":"markdown","source":["Now let us define a function to calculate the predictions  by forward propagation"]},{"metadata":{"id":"LxnLQL9-bT2A","colab_type":"code","colab":{}},"cell_type":"code","source":["def test(model, x, y):\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    # Forward propagation to calculate predictions\n","    _, probs = feedforward(model, x)\n","    preds = np.argmax(probs, axis=1)\n","    return np.count_nonzero(y==preds)/y.shape[0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PJd-MKKedKG1","colab_type":"text"},"cell_type":"markdown","source":["Now let us define a function to train the model. First we will perform forward propagation then backpropagation to update the gradient descent parameters then assign updated paramters to the model."]},{"metadata":{"id":"JneN3n5kiu12","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(model, X_train, X_test, Y_train, Y_test, print_loss=True):\n","    # Gradient descent. For each batch...\n","    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n","    for i in range(0, params[\"max_iter\"]):\n","\n","        # Forward propagation\n","        a1, probs = feedforward(model, X_train)\n","\n","        # Backpropagation\n","        dW2, db2, dW1, db1 = backpropagation(model, X_train, Y_train, a1, probs)\n","\n","        # Gradient descent parameter update\n","        W1 += -params[\"lr\"] * dW1\n","        b1 += -params[\"lr\"] * db1\n","        W2 += -params[\"lr\"] * dW2\n","        b2 += -params[\"lr\"] * db2\n","        \n","        # Assign new parameters to the model\n","        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n","        if print_loss and i % 50 == 0:\n","            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model, X_train, Y_train)),\n","                  \", Test accuracy:\", test(model, X_test, Y_test), \"\\n\")\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P0MI7Wvdiu16","colab_type":"text"},"cell_type":"markdown","source":["#### 3. Training the model"]},{"metadata":{"id":"x955E_rOiu17","colab_type":"code","outputId":"2cba2656-12e5-41d9-e682-6d263f39acf7","executionInfo":{"status":"ok","timestamp":1556355066979,"user_tz":-330,"elapsed":5429,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":708}},"cell_type":"code","source":["model = build_model()\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)\n","\n","model = train(model, X_train, X_test, Y_train, Y_test)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Loss after iteration 0: 2.492749 , Test accuracy: 0.13236929922135707 \n","\n","Loss after iteration 50: 1.712895 , Test accuracy: 0.45050055617352613 \n","\n","Loss after iteration 100: 1.311046 , Test accuracy: 0.6596218020022246 \n","\n","Loss after iteration 150: 1.043388 , Test accuracy: 0.7686318131256952 \n","\n","Loss after iteration 200: 0.858650 , Test accuracy: 0.8131256952169077 \n","\n","Loss after iteration 250: 0.725068 , Test accuracy: 0.8498331479421579 \n","\n","Loss after iteration 300: 0.625338 , Test accuracy: 0.8654060066740823 \n","\n","Loss after iteration 350: 0.548757 , Test accuracy: 0.882091212458287 \n","\n","Loss after iteration 400: 0.486890 , Test accuracy: 0.8921023359288098 \n","\n","Loss after iteration 450: 0.436110 , Test accuracy: 0.8987764182424917 \n","\n","Loss after iteration 500: 0.392825 , Test accuracy: 0.9087875417130145 \n","\n","Loss after iteration 550: 0.355599 , Test accuracy: 0.917686318131257 \n","\n","Loss after iteration 600: 0.324155 , Test accuracy: 0.9210233592880979 \n","\n","Loss after iteration 650: 0.297017 , Test accuracy: 0.9221357063403782 \n","\n","Loss after iteration 700: 0.273805 , Test accuracy: 0.9254727474972191 \n","\n","Loss after iteration 750: 0.253553 , Test accuracy: 0.92880978865406 \n","\n","Loss after iteration 800: 0.235535 , Test accuracy: 0.92880978865406 \n","\n","Loss after iteration 850: 0.219625 , Test accuracy: 0.9299221357063404 \n","\n","Loss after iteration 900: 0.205759 , Test accuracy: 0.932146829810901 \n","\n","Loss after iteration 950: 0.193485 , Test accuracy: 0.9354838709677419 \n","\n"],"name":"stdout"}]},{"metadata":{"id":"CSGc4tMlkYKY","colab_type":"text"},"cell_type":"markdown","source":["### Please answer the questions below to complete the experiment:"]},{"metadata":{"id":"0dguxf1Liu2B","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Good and Challenging me\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dBmeI-rgkbfG","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"test\" #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"20mhCAEFkdWE","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"Yes\" #@param [\"Yes\", \"No\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J3o516AOkfkp","colab_type":"code","cellView":"both","outputId":"0c0ef3f1-a93e-42f4-c395-d99aa801bf08","executionInfo":{"status":"ok","timestamp":1556355088557,"user_tz":-330,"elapsed":1355,"user":{"displayName":"Arjun Gupta","photoUrl":"https://lh3.googleusercontent.com/-79xPDqhG3Ck/AAAAAAAAAAI/AAAAAAAAAHk/eNdIlCuxOFA/s64/photo.jpg","userId":"06632186555192968294"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"cell_type":"code","source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Your submission is successful.\n","Ref Id: 4275\n","Date of submission:  27 Apr 2019\n","Time of submission:  14:18:57\n","View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\n","For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\n"],"name":"stdout"}]}]}